{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INCREASED PREDICTION ACCURACY IN THE GAME OF CRICKET \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Dataset IPL Player Stats - 2016 till 2019 taken from Kaggle\n",
    "https://www.kaggle.com/cclayford/cricinfo-statsguru-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. [Dataset Description](#cricket)\n",
    "2. [Importing the packages and dataset](#packages)\n",
    "3. [Exploring the dataset](#explore)\n",
    "4. [Data Cleaning](#clean)\n",
    "5. [Creating Additional Columns (or) Derived Attributes](#add)\n",
    "6. [Treating Category and Object values](#le)\n",
    "7. [Splitting data for Runs prediction](#rsplit)\n",
    "8. [Models for Runs Prediction](#rm)\n",
    "     - 8.1 [Decision Tree Classifier](#dtr)\n",
    "     - 8.2 [Pruned Decision Tree Classifier](#dtrprune)\n",
    "     - 8.3 [Random Forest Classifier](#rfr)\n",
    "     - 8.4 [Pruned Random Forest Classifier](#rfrprune)\n",
    "     - 8.5 [Gradient Boost Classifier](#gbr)\n",
    "     - 8.6 [Pruned Gradient Boosting Classifier](#gbrprune)\n",
    "     - 8.7 [Support Vector Machine Classifier](#svmr)\n",
    "     - 8.8 [KNearest Neighbors Classifier](#knnr)\n",
    "     - 8.9 [Bagging Classifier](#bagr)\n",
    "     - 8.10 [XG Boost Classifier](#xgbr)\n",
    "     - 8.11 [Pruned XG Boost Classifier](#xgbrprune)\n",
    "     - 8.12 [Ada Boost Classifier](#abr)\n",
    "     - 8.13 [Pruned Ada Boost Classifier](#abrprune)\n",
    "     - 8.14 [Logistic Regression](#lrr)\n",
    "     - 8.15 [Stacking  for Logistic Regression using Mlxtend Classifier](#strmlx)\n",
    "     - 8.16 [Stacking using Voting Classifier](#strvote)\n",
    "     - 8.17 [Comparison Table of all models for Runs Prediction](#ctr)\n",
    "9. [Splitting Data for Wickets Prediction](#wsplit)\n",
    "10. [Models for Wickets Prediction](#wm)\n",
    "     - 10.1 [Decision Tree Classifier](#dtw)\n",
    "     - 10.2 [Pruned Decision Tree Classifier](#dtwprune)\n",
    "     - 10.3 [Random Forest Classifier](#rfw)\n",
    "     - 10.4 [Gradient Boost Classifier](#gbw)\n",
    "     - 10.5 [Pruned Gradient Boosting Classifier](#gbwprune)\n",
    "     - 10.6 [Support Vector Machine Classifier](#svmw)\n",
    "     - 10.7 [KNearest Neighbors Classifier](#knnw)\n",
    "     - 10.8 [Bagging Classifier](#bagw)\n",
    "     - 10.9 [XG Boost Classifier](#xgbw)\n",
    "     - 10.10 [Pruned XG Boost Classifier](#xgbwprune)\n",
    "     - 10.11 [Ada Boost Classifier](#abw)\n",
    "     - 10.12 [Pruned Ada Boost Classifier](#abwprune)\n",
    "     - 10.13 [Logistic Regression](#lrw)\n",
    "     - 10.14 [Stacking  for Logistic Regression using Mlxtend Classifier](#stwmlx)\n",
    "     - 10.15 [Stacking using Voting Classifier](#stwvote)\n",
    "     - 10.16 [Comparison Table of all models for Runs Prediction](#ctw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset Description  <a id='cricket'>\n",
    "    \n",
    "**Objective**\n",
    "Our objective is to predict the performance of players using the columns Runs and Wickets.\n",
    "\n",
    "**Columns**\n",
    "\n",
    "1. Team - The Player Teams\n",
    "\n",
    "2. Player - Name of Players\n",
    "\n",
    "3. Tournament - Tournament Name\n",
    "\n",
    "4. Matches - No of Matches\n",
    "\n",
    "5. Batting Innings - No of innings the batsmen batted\n",
    "\n",
    "6. Not Out - No of times the batsmen was not out\n",
    "\n",
    "7. Runs Scored - Runs scored by the batsmen\n",
    "\n",
    "8. Highest Score - Highest score of the batsmen\n",
    "\n",
    "9. Batting Average -  Average number of runs scored by batsmen\n",
    "\n",
    "10. Balls Faced - Balls Faced by the batsmen\n",
    "\n",
    "11. Batting Strike Rate - Average no of Runs scored by batsmen per 100 balls\n",
    "\n",
    "12. 100 - No of Centuries scored by batsmen\n",
    "\n",
    "13. 50 - No of Fifties scored by batsmen\n",
    "\n",
    "14. 0 - No of zeroes scored by batsmen\n",
    "\n",
    "15. 4s - No of Fours batsmen scored\n",
    "\n",
    "16. 6s - No of Sixes batsmen scored\n",
    "\n",
    "17. Bowling Innings - No of innings the bowler bowled\n",
    "\n",
    "18. Overs Bowled - No of Overs the bowler bowled\n",
    "\n",
    "19. Maidens Bowled - No of Maidens the bowler bowled\n",
    "\n",
    "20. Runs Conceded - Total no of runs scored by opponent when bowler bowled\n",
    "\n",
    "21. Wickets Taken - No of Wickets taken by the bowler\n",
    "\n",
    "22. 4+ Innings Wickets - No of innings where bowler took more than four wickets\n",
    "\n",
    "23. 5+ Innings Wickets - No of innings where bowler took more than five wickets\n",
    "\n",
    "24. Catches Taken - No of catches taken\n",
    "\n",
    "25. Stumpings made - No of Stumpings made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing the packages and dataset  <a id='packages'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "cricket = pd.read_csv('IPL Player Stats - 2016 till 2019.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploring the dataset  <a id='explore'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team</th>\n",
       "      <th>Player</th>\n",
       "      <th>Tournament</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Batting Innings</th>\n",
       "      <th>Not Out</th>\n",
       "      <th>Runds Scored</th>\n",
       "      <th>Highest Score</th>\n",
       "      <th>Batting Average</th>\n",
       "      <th>Balls Faced</th>\n",
       "      <th>...</th>\n",
       "      <th>Runs Conceded</th>\n",
       "      <th>Wickets Taken</th>\n",
       "      <th>Best Bowling Figures</th>\n",
       "      <th>Bowling Average</th>\n",
       "      <th>Bowling Economy Rate</th>\n",
       "      <th>Bowling Strike Rate</th>\n",
       "      <th>4+ Innings Wickets</th>\n",
       "      <th>5+ Innings Wickets</th>\n",
       "      <th>Catches Taken</th>\n",
       "      <th>Stumpings Made</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delhi Daredevils</td>\n",
       "      <td>CH Morris</td>\n",
       "      <td>IPL 2016</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>195</td>\n",
       "      <td>82*</td>\n",
       "      <td>65.00</td>\n",
       "      <td>109</td>\n",
       "      <td>...</td>\n",
       "      <td>308</td>\n",
       "      <td>13</td>\n",
       "      <td>2/30</td>\n",
       "      <td>23.69</td>\n",
       "      <td>7.00</td>\n",
       "      <td>20.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi Daredevils</td>\n",
       "      <td>CH Morris</td>\n",
       "      <td>IPL 2017</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>154</td>\n",
       "      <td>52*</td>\n",
       "      <td>30.80</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>240</td>\n",
       "      <td>12</td>\n",
       "      <td>4/26</td>\n",
       "      <td>20.00</td>\n",
       "      <td>7.74</td>\n",
       "      <td>15.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhi Daredevils</td>\n",
       "      <td>CH Morris</td>\n",
       "      <td>IPL 2018</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>27*</td>\n",
       "      <td>46.00</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>143</td>\n",
       "      <td>3</td>\n",
       "      <td>2/41</td>\n",
       "      <td>47.66</td>\n",
       "      <td>10.21</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delhi Daredevils</td>\n",
       "      <td>JP Duminy</td>\n",
       "      <td>IPL 2016</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>191</td>\n",
       "      <td>49*</td>\n",
       "      <td>38.20</td>\n",
       "      <td>156</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>1/4</td>\n",
       "      <td>27.50</td>\n",
       "      <td>7.85</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delhi Daredevils</td>\n",
       "      <td>Q de Kock</td>\n",
       "      <td>IPL 2016</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>445</td>\n",
       "      <td>108</td>\n",
       "      <td>37.08</td>\n",
       "      <td>327</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Team     Player Tournament  Matches Batting Innings Not Out  \\\n",
       "0  Delhi Daredevils  CH Morris   IPL 2016       12               7       4   \n",
       "1  Delhi Daredevils  CH Morris   IPL 2017        9               9       4   \n",
       "2  Delhi Daredevils  CH Morris   IPL 2018        4               4       3   \n",
       "3  Delhi Daredevils  JP Duminy   IPL 2016       10               8       3   \n",
       "4  Delhi Daredevils  Q de Kock   IPL 2016       13              13       1   \n",
       "\n",
       "  Runds Scored Highest Score Batting Average Balls Faced  ... Runs Conceded  \\\n",
       "0          195           82*           65.00         109  ...           308   \n",
       "1          154           52*           30.80          94  ...           240   \n",
       "2           46           27*           46.00          26  ...           143   \n",
       "3          191           49*           38.20         156  ...            55   \n",
       "4          445           108           37.08         327  ...             -   \n",
       "\n",
       "  Wickets Taken Best Bowling Figures Bowling Average Bowling Economy Rate  \\\n",
       "0            13                 2/30           23.69                 7.00   \n",
       "1            12                 4/26           20.00                 7.74   \n",
       "2             3                 2/41           47.66                10.21   \n",
       "3             2                  1/4           27.50                 7.85   \n",
       "4             -                    -               -                    -   \n",
       "\n",
       "  Bowling Strike Rate 4+ Innings Wickets 5+ Innings Wickets Catches Taken  \\\n",
       "0                20.3                  0                  0             8   \n",
       "1                15.5                  1                  0             5   \n",
       "2                28.0                  0                  0             2   \n",
       "3                21.0                  0                  0             3   \n",
       "4                   -                  -                  -             2   \n",
       "\n",
       "  Stumpings Made  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              2  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Head of the dataset\n",
    "cricket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(631, 29)"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the dataset\n",
    "cricket.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Team', 'Player', 'Tournament', 'Matches', 'Batting Innings', 'Not Out',\n",
       "       'Runds Scored', 'Highest Score', 'Batting Average', 'Balls Faced',\n",
       "       'Batting Strike Rate', '100', '50', '0', '4s', '6s', 'Bowling Innings',\n",
       "       'Overs Bowled', 'Maidens Bowled', 'Runs Conceded', 'Wickets Taken',\n",
       "       'Best Bowling Figures', 'Bowling Average', 'Bowling Economy Rate',\n",
       "       'Bowling Strike Rate', '4+ Innings Wickets', '5+ Innings Wickets',\n",
       "       'Catches Taken', 'Stumpings Made'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Head of the dataset\n",
    "cricket.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns\n",
    "cricket = cricket.rename(columns={'Runds Scored':'Runs Scored','50':'Fifties','100':'Centuries','0':'Zeroes','Overs Bowled':'Overs','4s':'Fours','6s':'Sixes'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Team                    0\n",
       "Player                  0\n",
       "Tournament              0\n",
       "Matches                 0\n",
       "Batting Innings         0\n",
       "Not Out                 0\n",
       "Runs Scored             0\n",
       "Highest Score           0\n",
       "Batting Average         0\n",
       "Balls Faced             0\n",
       "Batting Strike Rate     0\n",
       "Centuries               0\n",
       "Fifties                 0\n",
       "Zeroes                  0\n",
       "Fours                   0\n",
       "Sixes                   0\n",
       "Bowling Innings         0\n",
       "Overs                   0\n",
       "Maidens Bowled          0\n",
       "Runs Conceded           0\n",
       "Wickets Taken           0\n",
       "Best Bowling Figures    0\n",
       "Bowling Average         0\n",
       "Bowling Economy Rate    0\n",
       "Bowling Strike Rate     0\n",
       "4+ Innings Wickets      0\n",
       "5+ Innings Wickets      0\n",
       "Catches Taken           0\n",
       "Stumpings Made          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for null values\n",
    "cricket.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 631 entries, 0 to 630\n",
      "Data columns (total 29 columns):\n",
      "Team                    631 non-null object\n",
      "Player                  631 non-null object\n",
      "Tournament              631 non-null object\n",
      "Matches                 631 non-null int64\n",
      "Batting Innings         631 non-null object\n",
      "Not Out                 631 non-null object\n",
      "Runs Scored             631 non-null object\n",
      "Highest Score           631 non-null object\n",
      "Batting Average         631 non-null object\n",
      "Balls Faced             631 non-null object\n",
      "Batting Strike Rate     631 non-null object\n",
      "Centuries               631 non-null object\n",
      "Fifties                 631 non-null object\n",
      "Zeroes                  631 non-null object\n",
      "Fours                   631 non-null object\n",
      "Sixes                   631 non-null object\n",
      "Bowling Innings         631 non-null object\n",
      "Overs                   631 non-null object\n",
      "Maidens Bowled          631 non-null object\n",
      "Runs Conceded           631 non-null object\n",
      "Wickets Taken           631 non-null object\n",
      "Best Bowling Figures    631 non-null object\n",
      "Bowling Average         631 non-null object\n",
      "Bowling Economy Rate    631 non-null object\n",
      "Bowling Strike Rate     631 non-null object\n",
      "4+ Innings Wickets      631 non-null object\n",
      "5+ Innings Wickets      631 non-null object\n",
      "Catches Taken           631 non-null int64\n",
      "Stumpings Made          631 non-null int64\n",
      "dtypes: int64(3), object(26)\n",
      "memory usage: 143.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Info of the dataset\n",
    "cricket.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Royal Challengers Bangalore    84\n",
       "Kings XI Punjab                79\n",
       "Sunrisers Hyderabad            77\n",
       "Kolkata Knight Riders          74\n",
       "Mumbai Indians                 73\n",
       "Delhi Daredevils               59\n",
       "Gujarat Lions                  43\n",
       "Rajasthan Royals               40\n",
       "Chennai Super Kings            39\n",
       "Rising Pune Supergiants        23\n",
       "Delhi Capitals                 20\n",
       "Rising Pune Supergiant         20\n",
       "Name: Team, dtype: int64"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Team column\n",
    "cricket['Team'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    103\n",
       "2     62\n",
       "-     62\n",
       "3     60\n",
       "4     52\n",
       "Name: Batting Innings, dtype: int64"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Batting Innings column\n",
    "cricket['Batting Innings'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    179\n",
       "0    173\n",
       "2     88\n",
       "3     68\n",
       "-     62\n",
       "Name: Not Out, dtype: int64"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Not Out column\n",
    "cricket['Not Out'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-     62\n",
       "0*    20\n",
       "1*    17\n",
       "0     16\n",
       "5*    12\n",
       "Name: Highest Score, dtype: int64"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Highest Score column\n",
    "cricket['Highest Score'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see * present in the values and all the columns we have seen has - value present which needs to be treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-       130\n",
       "0.00     21\n",
       "1.00     16\n",
       "8.00     12\n",
       "6.00     12\n",
       "Name: Batting Average, dtype: int64"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Batting Average column\n",
    "cricket['Batting Average'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-    62\n",
       "3    20\n",
       "1    19\n",
       "4    18\n",
       "2    16\n",
       "Name: Balls Faced, dtype: int64"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Balls Faced column\n",
    "cricket['Balls Faced'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-         70\n",
       "0.00      28\n",
       "100.00    22\n",
       "50.00     14\n",
       "33.33      7\n",
       "Name: Batting Strike Rate, dtype: int64"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Batting Strike Rate column\n",
    "cricket['Batting Strike Rate'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    551\n",
       "-     62\n",
       "1     15\n",
       "2      2\n",
       "4      1\n",
       "Name: Centuries, dtype: int64"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Centuries column\n",
    "cricket['Centuries'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    393\n",
       "1     71\n",
       "-     62\n",
       "2     41\n",
       "3     32\n",
       "4     14\n",
       "5      9\n",
       "6      5\n",
       "8      2\n",
       "7      1\n",
       "9      1\n",
       "Name: Fifties, dtype: int64"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Fifties column\n",
    "cricket['Fifties'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    369\n",
       "1    154\n",
       "-     62\n",
       "2     32\n",
       "3     14\n",
       "Name: Zeroes, dtype: int64"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Zeroes column\n",
    "cricket['Zeroes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    151\n",
       "1     67\n",
       "-     62\n",
       "2     38\n",
       "3     26\n",
       "Name: Fours, dtype: int64"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Fours column (taken head due to presence of lot of values)\n",
    "cricket['Fours'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    199\n",
       "1     81\n",
       "-     62\n",
       "4     35\n",
       "2     29\n",
       "Name: Sixes, dtype: int64"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Sixes column (taken head due to presence of lot of values)\n",
    "cricket['Sixes'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-    218\n",
       "1     54\n",
       "2     46\n",
       "4     33\n",
       "3     33\n",
       "Name: Bowling Innings, dtype: int64"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Bowling Innings column (taken head due to presence of lot of values)\n",
    "cricket['Bowling Innings'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-      218\n",
       "2.0     20\n",
       "4.0     20\n",
       "3.0     16\n",
       "1.0     13\n",
       "Name: Overs, dtype: int64"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Overs column (taken head due to presence of lot of values)\n",
    "cricket['Overs'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    362\n",
       "-    218\n",
       "1     40\n",
       "2      8\n",
       "3      2\n",
       "Name: Maidens Bowled, dtype: int64"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Maidens Bowled column (taken head due to presence of lot of values)\n",
    "cricket['Maidens Bowled'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-     218\n",
       "42      6\n",
       "49      6\n",
       "31      5\n",
       "33      4\n",
       "Name: Runs Conceded, dtype: int64"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Runs Conceded column (taken head due to presence of lot of values)\n",
    "cricket['Runs Conceded'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-    218\n",
       "0     67\n",
       "1     47\n",
       "2     45\n",
       "3     35\n",
       "Name: Wickets Taken, dtype: int64"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Wickets Taken column (taken head due to presence of lot of values)\n",
    "cricket['Wickets Taken'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-        285\n",
       "21.00      5\n",
       "27.00      5\n",
       "35.50      4\n",
       "33.00      4\n",
       "Name: Bowling Average, dtype: int64"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Bowling Average column (taken head due to presence of lot of values)\n",
    "cricket['Bowling Average'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-        218\n",
       "10.50      8\n",
       "9.00       8\n",
       "8.00       7\n",
       "13.00      5\n",
       "Name: Bowling Economy Rate, dtype: int64"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Bowling Economy Rate column (taken head due to presence of lot of values)\n",
    "cricket['Bowling Economy Rate'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-       285\n",
       "12.0     16\n",
       "24.0     16\n",
       "30.0     11\n",
       "21.0     10\n",
       "Name: Bowling Strike Rate, dtype: int64"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Bowling Strike Rate column (taken head due to presence of lot of values)\n",
    "cricket['Bowling Strike Rate'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    381\n",
       "-    218\n",
       "1     28\n",
       "2      3\n",
       "3      1\n",
       "Name: 4+ Innings Wickets, dtype: int64"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for 4+ Innings Wickets column (taken head due to presence of lot of values)\n",
    "cricket['4+ Innings Wickets'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    407\n",
       "-    218\n",
       "1      6\n",
       "Name: 5+ Innings Wickets, dtype: int64"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for 5+ Innings Wickets column (taken head due to presence of lot of values)\n",
    "cricket['5+ Innings Wickets'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Cleaning  <a id='clean'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the '*' values from the Highest Score column\n",
    "cricket['Highest Score'] = cricket['Highest Score'].apply(lambda x:x.replace('*',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-      62\n",
       "0      36\n",
       "1      28\n",
       "7      21\n",
       "5      19\n",
       "6      16\n",
       "2      15\n",
       "4      12\n",
       "3      12\n",
       "12     11\n",
       "9      11\n",
       "15     10\n",
       "8      10\n",
       "10     10\n",
       "19      9\n",
       "16      9\n",
       "35      8\n",
       "27      7\n",
       "24      7\n",
       "65      7\n",
       "67      7\n",
       "31      7\n",
       "36      7\n",
       "11      7\n",
       "13      7\n",
       "52      6\n",
       "21      6\n",
       "34      6\n",
       "50      6\n",
       "14      6\n",
       "       ..\n",
       "90      2\n",
       "104     2\n",
       "85      2\n",
       "80      2\n",
       "48      2\n",
       "29      2\n",
       "58      2\n",
       "73      2\n",
       "102     2\n",
       "23      1\n",
       "113     1\n",
       "126     1\n",
       "79      1\n",
       "101     1\n",
       "108     1\n",
       "71      1\n",
       "128     1\n",
       "129     1\n",
       "86      1\n",
       "103     1\n",
       "42      1\n",
       "117     1\n",
       "114     1\n",
       "94      1\n",
       "87      1\n",
       "78      1\n",
       "38      1\n",
       "49      1\n",
       "105     1\n",
       "88      1\n",
       "Name: Highest Score, Length: 113, dtype: int64"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts of Highest Score column\n",
    "cricket['Highest Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that the '*' values have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to remove all the '-' values present in all the columns\n",
    "def removedash(a):\n",
    "    a = a.replace('-',np.nan)\n",
    "    a = a.astype(float)\n",
    "    a = a.fillna(value=a.median())\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the function for each columns to remove the '-' value\n",
    "cricket['5+ Innings Wickets'] = removedash(cricket['5+ Innings Wickets'])\n",
    "\n",
    "cricket['4+ Innings Wickets'] = removedash(cricket['4+ Innings Wickets'])\n",
    "\n",
    "cricket['Bowling Strike Rate'] = removedash(cricket['Bowling Strike Rate'])\n",
    "\n",
    "cricket['Bowling Economy Rate'] = removedash(cricket['Bowling Economy Rate'])\n",
    "\n",
    "cricket['Bowling Average'] = removedash(cricket['Bowling Average'])\n",
    "\n",
    "cricket['Wickets Taken'] = removedash(cricket['Wickets Taken'])\n",
    "\n",
    "cricket['Runs Conceded'] = removedash(cricket['Runs Conceded'])\n",
    "\n",
    "cricket['Maidens Bowled'] = removedash(cricket['Maidens Bowled'])\n",
    "\n",
    "cricket['Overs'] = removedash(cricket['Overs'])\n",
    "\n",
    "cricket['Bowling Innings'] = removedash(cricket['Bowling Innings'])\n",
    "\n",
    "cricket['Fours'] = removedash(cricket['Fours'])\n",
    "\n",
    "cricket['Sixes'] = removedash(cricket['Sixes'])\n",
    "\n",
    "cricket['Zeroes'] = removedash(cricket['Zeroes'])\n",
    "\n",
    "cricket['Fifties'] = removedash(cricket['Fifties'])\n",
    "\n",
    "cricket['Centuries'] = removedash(cricket['Centuries'])\n",
    "\n",
    "cricket['Batting Strike Rate'] = removedash(cricket['Batting Strike Rate'])\n",
    "\n",
    "cricket['Balls Faced'] = removedash(cricket['Balls Faced'])\n",
    "\n",
    "cricket['Batting Average'] = removedash(cricket['Batting Average'])\n",
    "\n",
    "cricket['Not Out'] = removedash(cricket['Not Out'])\n",
    "\n",
    "cricket['Highest Score'] = removedash(cricket['Highest Score'])\n",
    "\n",
    "cricket['Batting Innings'] = removedash(cricket['Batting Innings'])\n",
    "\n",
    "cricket['Runs Scored'] = removedash(cricket['Runs Scored'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(631, 29)"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the dataset after Data Cleaning\n",
    "cricket.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the column as I am not able to understand\n",
    "cricket = cricket.drop('Best Bowling Figures',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating Additional Columns (or) Derived Attributes<a id='add'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now creating additional columns from the existing columns to derive more insights, the columns to be created are as follows:\n",
    "    \n",
    "    1. Batting Consistency - Tells how consistent the batsmen are\n",
    "    \n",
    "    2. Bowling Consistency - Tells how consistent the bowlers are\n",
    "    \n",
    "    3. Batting Form - Tells what form the batsmen are\n",
    "    \n",
    "    4. Bowling Form - Tells what form the bowlers are\n",
    "    \n",
    "    5. Batting Opposition - Describes the batsmens performance against a opposite team\n",
    "    \n",
    "    6. Bowling Opposition - Describes the bowlers performance against a opposite team\n",
    "    \n",
    "    7. Batting Venue - Describes the batsmen performance at a venue\n",
    "    \n",
    "    8. Bowling Venue - Describes the bowlers performance at a venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Batting Consistency column using the formula\n",
    "cricket['Batting Consistency'] = (0.2566*cricket['Batting Innings'])+(0.1510*cricket['Batting Strike Rate'])+(0.0787*cricket['Centuries'])+(0.0556*cricket['Fifties'])-(0.0328*cricket['Zeroes']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Bowling Consistency column using the formula\n",
    "cricket['Bowling Consistency'] = 0.4174 * cricket['Overs']+ 0.2634 * cricket['Bowling Innings'] + 0.1602 * cricket['Bowling Strike Rate'] + 0.0975*cricket['Bowling Average'] + 0.0615*(cricket['4+ Innings Wickets'] + cricket['5+ Innings Wickets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Batting Form column using the formula\n",
    "cricket['Batting Form'] = (0.4262*cricket['Batting Average'])+(0.2566*cricket['Batting Innings'])+(0.1510*cricket['Batting Strike Rate'])+(0.0787*cricket['Centuries'])+(0.0556*cricket['Fifties'])-(0.0328*cricket['Zeroes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Bowling Form column using the formula\n",
    "cricket['Bowling Form'] =  0.3269*cricket['Overs'] + 0.2846*cricket['Bowling Innings'] + 0.1877*cricket['Bowling Strike Rate'] + 0.1210*cricket['Bowling Average'] + 0.0798*(cricket['4+ Innings Wickets']+cricket['5+ Innings Wickets']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Batting Opposition column using the formula\n",
    "cricket['Batting Opposition'] = (0.4262*cricket['Batting Average'])+(0.2566*cricket['Batting Innings'])+(0.1510*cricket['Batting Strike Rate'])+(0.0787*cricket['Centuries'])+(0.0556*cricket['Fifties'])-(0.0328*cricket['Zeroes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Bowling Opposition column using the formula\n",
    "cricket['Bowling Opposition'] =  (0.3177*cricket['Overs'])+(0.3177*cricket['Bowling Innings'])+(0.1933*cricket['Bowling Strike Rate'])+(0.1465*cricket['Bowling Average'])+(0.0943*(cricket['4+ Innings Wickets']+cricket['5+ Innings Wickets']))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Batting Venue column using the formula\n",
    "cricket['Batting Venue'] = (0.4262*cricket['Batting Average'])+(0.2566*cricket['Batting Innings'])+(0.1510*cricket['Batting Strike Rate'])+(0.0787*cricket['Centuries'])+(0.0556*cricket['Fifties'])+(0.0328*cricket['Highest Score']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Bowling Venue column using the formula\n",
    "cricket['Bowling Venue'] = (0.3018*cricket['Overs'])+(0.2783*cricket['Bowling Innings'])+(0.1836*cricket['Bowling Strike Rate'])+(0.1391*cricket['Bowling Average'])+(0.0972*(cricket['4+ Innings Wickets']+cricket['5+ Innings Wickets'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning the Batting Average columns to create a new column Batting Average Rating\n",
    "bins = [0.00,29.99,49.99,69.99,89.99]\n",
    "names = ['0.00-29.99','30.00-49.99','50.00-69.99','70.00-89.99']\n",
    "\n",
    "cricket['Batting Average Rating'] = pd.cut(cricket['Batting Average'], bins, labels=names,include_lowest=True)\n",
    "\n",
    "replace_batting = {'0.00-29.99':'1','30.00-49.99':'2','50.00-69.99':'3','70.00-89.99':'4'}\n",
    "\n",
    "cricket = cricket.replace({'Batting Average Rating':replace_batting})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    511\n",
       "2     96\n",
       "3     20\n",
       "4      4\n",
       "Name: Batting Average Rating, dtype: int64"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Batting Average Rating column\n",
    "cricket['Batting Average Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning the Batting Strike Rate column to create a new column Batting SR Rating\n",
    "bins = [0,100.00,200.00,300.00,400.00]\n",
    "names = ['0.00-100.00','101.00-200.00','201.00-300.00','301.00-400.00']\n",
    "\n",
    "cricket['Batting SR Rating'] = pd.cut(cricket['Batting Strike Rate'],bins,labels=names,include_lowest=True,)\n",
    "\n",
    "replace_battingstrike = {'0.00-100.00':'1','101.00-200.00':'2','201.00-300.00':'3','301.00-400.00':'4'}\n",
    "\n",
    "cricket = cricket.replace({'Batting SR Rating':replace_battingstrike})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    424\n",
       "1    194\n",
       "3     12\n",
       "4      1\n",
       "Name: Batting SR Rating, dtype: int64"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Batting SR Rating column\n",
    "cricket['Batting SR Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning the Bowling Average column to create a new column Bowling Average Rating\n",
    "bins = [0,50.00,100.00,150.00,200.00,250.00]\n",
    "names = ['0.00-50.00','51.00-100.00','101.00-150.00','151.00-200.00','201.00-250.00']\n",
    "\n",
    "cricket['Bowling Average Rating'] = pd.cut(cricket['Bowling Average'],bins,labels=names)\n",
    "\n",
    "replace_bowling = {'0.00-50.00':'1','51.00-100.00':'2','101.00-150.00':'3','151.00-200.00':'4','201.00-250.00':'5'}\n",
    "\n",
    "cricket = cricket.replace({'Bowling Average Rating':replace_bowling})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    570\n",
       "2     50\n",
       "3      9\n",
       "5      1\n",
       "4      1\n",
       "Name: Bowling Average Rating, dtype: int64"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Bowling Average Rating column\n",
    "cricket['Bowling Average Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning the Bowling Strike Rate column to create a new column Bowling SR Rating\n",
    "bins = [0.00,50.00,100.00,150.00,200.00]\n",
    "names = ['0.00-50.00','51.00-100.00','101.00-150.00','151.00-200.00']\n",
    "\n",
    "cricket['Bowling SR Rating'] = pd.cut(cricket['Bowling Strike Rate'],bins,labels=names)\n",
    "\n",
    "replace_bowlingSR = {'0.00-50.00':'1','51.00-100.00':'2','101.00-150.00':'3','151.00-200.00':'4'}\n",
    "\n",
    "cricket = cricket.replace({'Bowling SR Rating':replace_bowlingSR})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    610\n",
       "2     18\n",
       "3      3\n",
       "Name: Bowling SR Rating, dtype: int64"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Bowling SR Rating column\n",
    "cricket['Bowling SR Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the values present in these columns are with small differences and most of the values are same, we have binned to rate them\n",
    "cricket = cricket.drop('Bowling Strike Rate',1)\n",
    "cricket = cricket.drop('Batting Average',1)\n",
    "cricket = cricket.drop('Bowling Average',1)\n",
    "cricket = cricket.drop('Batting Strike Rate',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning the Runs Scored column to create a new column Runs\n",
    "bins = [0.0,500.00,1000.00]\n",
    "names = ['0.0-600.0','601.0-1200.0']\n",
    "\n",
    "cricket['Runs'] = pd.cut(cricket['Runs Scored'],bins,labels=names,include_lowest=True)\n",
    "\n",
    "replace_runs = {'0.0-600.0':'1','601.0-1200.0':'2'}\n",
    "\n",
    "cricket = cricket.replace({'Runs':replace_runs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    612\n",
       "2     19\n",
       "Name: Runs, dtype: int64"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Runs column\n",
    "cricket['Runs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning the Wickets Taken to create a new column Wickets\n",
    "bins = [0.0,15.0,30.0]\n",
    "names = ['0.0-15.0','16.0-30.0']\n",
    "\n",
    "cricket['Wickets'] = pd.cut(cricket['Wickets Taken'],bins,labels=names,include_lowest=True)\n",
    "\n",
    "replace_wickets = {'0.0-15.0':'1','16.0-30.0':'2'}\n",
    "\n",
    "cricket = cricket.replace({'Wickets':replace_wickets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    592\n",
       "2     39\n",
       "Name: Wickets, dtype: int64"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for Wickets column\n",
    "cricket['Wickets'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the Runs Scored and Wickets Taken column as we have created new columns Runs and Wickets by rating them\n",
    "cricket = cricket.drop('Runs Scored',1)\n",
    "cricket = cricket.drop('Wickets Taken',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 631 entries, 0 to 630\n",
      "Data columns (total 36 columns):\n",
      "Team                      631 non-null object\n",
      "Player                    631 non-null object\n",
      "Tournament                631 non-null object\n",
      "Matches                   631 non-null int64\n",
      "Batting Innings           631 non-null float64\n",
      "Not Out                   631 non-null float64\n",
      "Highest Score             631 non-null float64\n",
      "Balls Faced               631 non-null float64\n",
      "Centuries                 631 non-null float64\n",
      "Fifties                   631 non-null float64\n",
      "Zeroes                    631 non-null float64\n",
      "Fours                     631 non-null float64\n",
      "Sixes                     631 non-null float64\n",
      "Bowling Innings           631 non-null float64\n",
      "Overs                     631 non-null float64\n",
      "Maidens Bowled            631 non-null float64\n",
      "Runs Conceded             631 non-null float64\n",
      "Bowling Economy Rate      631 non-null float64\n",
      "4+ Innings Wickets        631 non-null float64\n",
      "5+ Innings Wickets        631 non-null float64\n",
      "Catches Taken             631 non-null int64\n",
      "Stumpings Made            631 non-null int64\n",
      "Batting Consistency       631 non-null float64\n",
      "Bowling Consistency       631 non-null float64\n",
      "Batting Form              631 non-null float64\n",
      "Bowling Form              631 non-null float64\n",
      "Batting Opposition        631 non-null float64\n",
      "Bowling Opposition        631 non-null float64\n",
      "Batting Venue             631 non-null float64\n",
      "Bowling Venue             631 non-null float64\n",
      "Batting Average Rating    631 non-null object\n",
      "Batting SR Rating         631 non-null object\n",
      "Bowling Average Rating    631 non-null object\n",
      "Bowling SR Rating         631 non-null object\n",
      "Runs                      631 non-null object\n",
      "Wickets                   631 non-null object\n",
      "dtypes: float64(24), int64(3), object(9)\n",
      "memory usage: 177.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Info of the dataset\n",
    "cricket.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Treating Category and Object values <a id='le'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Runs and Wickets from Object to Integer data type\n",
    "cricket['Runs'] = cricket['Runs'].astype(int)\n",
    "\n",
    "cricket['Wickets'] = cricket['Wickets'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Label Encoder and encoding Team, Tournament and Player columns using Label Encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "cricket['Team'] = le.fit_transform(cricket['Team'])\n",
    "\n",
    "cricket['Tournament'] = le.fit_transform(cricket['Tournament'])\n",
    "\n",
    "cricket['Player'] = le.fit_transform(cricket['Player'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Batting Average Rating, Batting SR Rating, Bowling SR Rating columns from object to integer data type\n",
    "cricket['Batting Average Rating'] = cricket['Batting Average Rating'].astype(int)\n",
    "\n",
    "cricket['Batting SR Rating'] = cricket['Batting SR Rating'].astype(int)\n",
    "\n",
    "cricket['Bowling Average Rating'] = cricket['Bowling Average Rating'].astype(int)\n",
    "\n",
    "cricket['Bowling SR Rating'] = cricket['Bowling SR Rating'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 631 entries, 0 to 630\n",
      "Data columns (total 36 columns):\n",
      "Team                      631 non-null int32\n",
      "Player                    631 non-null int32\n",
      "Tournament                631 non-null int32\n",
      "Matches                   631 non-null int64\n",
      "Batting Innings           631 non-null float64\n",
      "Not Out                   631 non-null float64\n",
      "Highest Score             631 non-null float64\n",
      "Balls Faced               631 non-null float64\n",
      "Centuries                 631 non-null float64\n",
      "Fifties                   631 non-null float64\n",
      "Zeroes                    631 non-null float64\n",
      "Fours                     631 non-null float64\n",
      "Sixes                     631 non-null float64\n",
      "Bowling Innings           631 non-null float64\n",
      "Overs                     631 non-null float64\n",
      "Maidens Bowled            631 non-null float64\n",
      "Runs Conceded             631 non-null float64\n",
      "Bowling Economy Rate      631 non-null float64\n",
      "4+ Innings Wickets        631 non-null float64\n",
      "5+ Innings Wickets        631 non-null float64\n",
      "Catches Taken             631 non-null int64\n",
      "Stumpings Made            631 non-null int64\n",
      "Batting Consistency       631 non-null float64\n",
      "Bowling Consistency       631 non-null float64\n",
      "Batting Form              631 non-null float64\n",
      "Bowling Form              631 non-null float64\n",
      "Batting Opposition        631 non-null float64\n",
      "Bowling Opposition        631 non-null float64\n",
      "Batting Venue             631 non-null float64\n",
      "Bowling Venue             631 non-null float64\n",
      "Batting Average Rating    631 non-null int32\n",
      "Batting SR Rating         631 non-null int32\n",
      "Bowling Average Rating    631 non-null int32\n",
      "Bowling SR Rating         631 non-null int32\n",
      "Runs                      631 non-null int32\n",
      "Wickets                   631 non-null int32\n",
      "dtypes: float64(24), int32(9), int64(3)\n",
      "memory usage: 155.4 KB\n"
     ]
    }
   ],
   "source": [
    "# Info of the dataset\n",
    "cricket.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Splitting data for Runs <a id = 'rsplit'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning X and y for train test split\n",
    "X_runs = cricket.drop('Runs',1)\n",
    "y_runs = cricket['Runs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing train test split and splitting the data with test size 0.33\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_runs,X_test_runs,y_train_runs,y_test_runs = train_test_split(X_runs,y_runs,random_state=2,test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Models for Runs Prediction <a id='rm'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Decision Tree Classiifier <a id = dtr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Decision Tree Classifier and fitting the Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here given class weight as balanced to balance the classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing the model\n",
    "dt.fit(X_train_runs,y_train_runs)\n",
    "\n",
    "dt_pred_runs = dt.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Metrics for model evaluation\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9617224880382775\n",
      "Precision Score: 0.9900497512437811\n",
      "Recall Score: 0.9707317073170731\n",
      "F1 Score: 0.9802955665024631\n",
      "ROC AUC Score: 0.7353658536585366\n",
      "Train Score for Decision Tree Classifier 1.0\n",
      "Test Score for Decision Tree Classifier 0.9617224880382775\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "dt_runs_accuracy = metrics.accuracy_score(dt_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',dt_runs_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "dt_runs_precision = metrics.precision_score(dt_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',dt_runs_precision)\n",
    "\n",
    "# Recall Score\n",
    "dt_runs_recall = metrics.recall_score(dt_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',dt_runs_recall)\n",
    "\n",
    "# F1 Score\n",
    "dt_runs_f1score = metrics.f1_score(dt_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',dt_runs_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "dt_runs_aucrocscore = metrics.roc_auc_score(dt_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',dt_runs_aucrocscore)\n",
    "\n",
    "# Train Score for Decision Tree Classifier\n",
    "dt_runs_train = dt.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score for Decision Tree Classifier\n",
    "print('Train Score for Decision Tree Classifier',dt_runs_train)\n",
    "\n",
    "# Test Score for Decision Tree Classifier\n",
    "dt_runs_test = dt.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score for Decision Tree Classifier\n",
    "print('Test Score for Decision Tree Classifier',dt_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that the scores are good but as the Train score is 1.0 we can confirm that the model is Overfitting\n",
    "\n",
    "To eliminate Overfitting we need to perform Pruning Techniques (ie) tune the parameters of the model\n",
    "\n",
    "The tuning of the parameters are done by Randomized Search CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Pruned Decision Tree Classifier<a id ='dtrprune'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion for tree\n",
    "criterion = ['gini','entropy']\n",
    "# Splitter for tree\n",
    "splitter = ['random','best']\n",
    "# Maximum levels of  trees\n",
    "max_depth = [2,5,10,15]\n",
    "# Maximum number of samples required to split a node\n",
    "max_leaf_nodes = [2, 5, 10]\n",
    "# Maximum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "\n",
    "# Parameter grid assigned for performing Randomized Sear\n",
    "random_grid = {'criterion': criterion,\n",
    "               'splitter': splitter,\n",
    "               'max_depth': max_depth,\n",
    "               'max_leaf_nodes': max_leaf_nodes,\n",
    "               'min_samples_leaf': min_samples_leaf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=DecisionTreeClassifier(class_weight='balanced'),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [2, 5, 10, 15],\n",
       "                                        'max_leaf_nodes': [2, 5, 10],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'splitter': ['random', 'best']},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Randomized Search CV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "dt_random = RandomizedSearchCV(estimator = dt, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "dt_random.fit(X_runs,y_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'splitter': 'random',\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_leaf_nodes': 10,\n",
       " 'max_depth': 10,\n",
       " 'criterion': 'entropy'}"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "dt_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model again after parameter tuning by assigning the parameters given by Randomized Search CV\n",
    "dt_prune = DecisionTreeClassifier(class_weight='balanced',criterion = \"entropy\", splitter = 'random', max_leaf_nodes = 10, min_samples_leaf = 1,max_depth= 10)\n",
    "\n",
    "dt_prune.fit(X_train_runs,y_train_runs)\n",
    "dt_prune_runs_pred = dt_prune.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score after Pruning: 0.9617224880382775\n",
      "Precision Score after Pruning: 0.9900497512437811\n",
      "Recall Score after Pruning: 0.9707317073170731\n",
      "F1 Score after Pruning: 0.9802955665024631\n",
      "ROC AUC Score after Pruning: 0.7353658536585366\n",
      "Train Score for Decision Tree Classifier after Pruning 0.9976303317535545\n",
      "Test Score for Decision Tree Classifier  0.9617224880382775\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score after Pruning\n",
    "dt_prune_runs_accuracy = metrics.accuracy_score(dt_prune_runs_pred,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score after Pruning\n",
    "print('Accuracy Score after Pruning:',dt_prune_runs_accuracy)\n",
    "\n",
    "# Precision Score after Pruning\n",
    "dt_prune_runs_precision = metrics.precision_score(dt_prune_runs_pred,y_test_runs)\n",
    "\n",
    "# Printing Precision Score after Pruning\n",
    "print('Precision Score after Pruning:',dt_prune_runs_precision)\n",
    "\n",
    "# Recall Score after Pruning\n",
    "dt_prune_runs_recall = metrics.recall_score(dt_prune_runs_pred,y_test_runs)\n",
    "\n",
    "# Printing Recall Score after Pruning\n",
    "print('Recall Score after Pruning:',dt_prune_runs_recall)\n",
    "\n",
    "# F1 Score after Pruning\n",
    "dt_prune_runs_f1score = metrics.f1_score(dt_prune_runs_pred,y_test_runs)\n",
    "\n",
    "# Printing F1 Score after Pruning\n",
    "print('F1 Score after Pruning:',dt_prune_runs_f1score)\n",
    "\n",
    "# ROC AUC Score after Pruning\n",
    "dt_prune_runs_aucrocscore = metrics.roc_auc_score(dt_prune_runs_pred,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score after Pruning\n",
    "print('ROC AUC Score after Pruning:',dt_prune_runs_aucrocscore)\n",
    "\n",
    "# Train Score for Decision Tree Classifier after Pruning\n",
    "dt_prune_runs_train = dt_prune.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score for Decision Tree Classifier after Pruning\n",
    "print('Train Score for Decision Tree Classifier after Pruning',dt_prune_runs_train)\n",
    "\n",
    "# Test Score for Decision Tree Classifier after Pruning\n",
    "dt_prune_runs_test = dt_prune.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score for Decision Tree Classifier after Pruning\n",
    "print('Test Score for Decision Tree Classifier ',dt_prune_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see after Pruning its not Overfitting like before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Random Forest Classifier <a id='rfr'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Random Forest Classifier and fitting, training and testing the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "rf.fit(X_train_runs,y_train_runs)\n",
    "rf_pred_runs = rf.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9760765550239234\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.9757281553398058\n",
      "F1 Score: 0.9877149877149877\n",
      "ROC AUC Score: 0.9878640776699029\n",
      "Train Score for Random Forest Classifier 1.0\n",
      "Test Score for Random Forest Classifier 0.9760765550239234\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "rf_runs_accuracy = metrics.accuracy_score(rf_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',rf_runs_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "rf_runs_precision = metrics.precision_score(rf_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',rf_runs_precision)\n",
    "\n",
    "# Recall Score\n",
    "rf_runs_recall = metrics.recall_score(rf_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',rf_runs_recall)\n",
    "\n",
    "# F1 Score\n",
    "rf_runs_f1score = metrics.f1_score(rf_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',rf_runs_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "rf_runs_aucrocscore = metrics.roc_auc_score(rf_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',rf_runs_aucrocscore)\n",
    "\n",
    "# Train Score for Random Forest Classifier\n",
    "rf_runs_train = rf.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score for Random Forest Classifier\n",
    "print('Train Score for Random Forest Classifier',rf_runs_train)\n",
    "\n",
    "# Test Score for Random Forest Classifier\n",
    "rf_runs_test = rf.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score for Random Forest Classifier\n",
    "print('Test Score for Random Forest Classifier',rf_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here too we could see Overfitting problem and also here for Precision the score is 1.0, maybe for this classifier there are no False Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Pruned Random Forest Classifier <a id='rfrprune'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No of estimators\n",
    "n_estimators=[1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [2,5,10,15]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Grid of parameters for Randomized Search CV\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 285 out of 300 | elapsed:    8.9s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   10.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=RandomForestClassifier(class_weight='balanced'),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [2, 5, 10, 15],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [1, 2, 4, 8, 16, 32, 64,\n",
       "                                                         100, 200]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_runs,y_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 8,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 10,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model,training and testing after parameter by assigning the parameters\n",
    "rf_prune = RandomForestClassifier(class_weight='balanced',n_estimators=8,min_samples_split=2,min_samples_leaf=4,max_features='auto',max_depth=10,bootstrap='True')\n",
    "\n",
    "rf_prune.fit(X_train_runs,y_train_runs)\n",
    "\n",
    "rf_prune_pred_runs = rf_prune.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score after Pruning: 0.9856459330143541\n",
      "Precision Score after Pruning: 0.9900497512437811\n",
      "Recall Score after Pruning: 0.995\n",
      "F1 Score after Pruning: 0.9925187032418954\n",
      "ROC AUC Score after Pruning: 0.8863888888888888\n",
      "Train Score for Random Forest Classifier after Pruning 0.990521327014218\n",
      "Test Score for Random Forest Classifier after Pruning 0.9856459330143541\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score after Pruning\n",
    "rf_prune_runs_accuracy = metrics.accuracy_score(rf_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score after Pruning\n",
    "print('Accuracy Score after Pruning:',rf_prune_runs_accuracy)\n",
    "\n",
    "# Precision Score after Pruning\n",
    "rf_prune_runs_precision = metrics.precision_score(rf_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Precision Score after Pruning\n",
    "print('Precision Score after Pruning:',rf_prune_runs_precision)\n",
    "\n",
    "# Recall Score after Pruning\n",
    "rf_prune_runs_recall = metrics.recall_score(rf_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Recall Score after Pruning\n",
    "print('Recall Score after Pruning:',rf_prune_runs_recall)\n",
    "\n",
    "# F1 Score after Pruning\n",
    "rf_prune_runs_f1score = metrics.f1_score(rf_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing F1 Score after Pruning\n",
    "print('F1 Score after Pruning:',rf_prune_runs_f1score)\n",
    "\n",
    "# ROC AUC Score after Pruning\n",
    "rf_prune_runs_aucrocscore = metrics.roc_auc_score(rf_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score after Pruning\n",
    "print('ROC AUC Score after Pruning:',rf_prune_runs_aucrocscore)\n",
    "\n",
    "# Train Score for Random Forest Classifier after Pruning\n",
    "rf_prune_runs_train = rf_prune.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score for Random Forest Classifier after Pruning\n",
    "print('Train Score for Random Forest Classifier after Pruning',rf_prune_runs_train)\n",
    "\n",
    "# Test Score for Random Forest Classifier after Pruning\n",
    "rf_prune_runs_test = rf_prune.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score for Random Forest Classifier after Pruning\n",
    "print('Test Score for Random Forest Classifier after Pruning',rf_prune_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see after Pruning its not Overfitting life before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Gradient Boosting Classifier <a id='gbr'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Gradient Booosting Classifier and fitting, training and testing the model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "gb.fit(X_train_runs,y_train_runs)\n",
    "gb_pred_runs = gb.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9856459330143541\n",
      "Precision Score: 0.9950248756218906\n",
      "Recall Score 0.9900990099009901\n",
      "F1 Score: 0.9925558312655087\n",
      "ROC AUC Score: 0.9236209335219236\n",
      "Train Score for Gradient Boosting Classifier 1.0\n",
      "Test Score for Gradient Boosting Classifier 0.9856459330143541\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "gb_runs_accuracy = metrics.accuracy_score(gb_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score \n",
    "print('Accuracy Score:',gb_runs_accuracy)\n",
    "\n",
    "# Precision Score \n",
    "gb_runs_precision = metrics.precision_score(gb_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Precision Score \n",
    "print('Precision Score:',gb_runs_precision)\n",
    "\n",
    "# Recall Score\n",
    "gb_runs_recall = metrics.recall_score(gb_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Recall Score \n",
    "print('Recall Score',gb_runs_recall)\n",
    "\n",
    "# F1 Score \n",
    "gb_runs_f1score = metrics.f1_score(gb_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing F1 Score \n",
    "print('F1 Score:',gb_runs_f1score)\n",
    "\n",
    "# ROC AUC Score \n",
    "gb_runs_aucrocscore = metrics.roc_auc_score(gb_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score \n",
    "print('ROC AUC Score:',gb_runs_aucrocscore)\n",
    "\n",
    "# Train Score for Gradient Boosting Classifier \n",
    "gb_runs_train = gb.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score for Gradient Boosting Classifier \n",
    "print('Train Score for Gradient Boosting Classifier',gb_runs_train)\n",
    "\n",
    "# Test Score for Gradient Boosting Classifier \n",
    "gb_runs_test = gb.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score for Gradient Boosting Classifier\n",
    "print('Test Score for Gradient Boosting Classifier',gb_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here too there is Overfitting which needs to be corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Pruned Gradient Boosting Classifier <a id='gbrprune'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "loss = ['deviance','exponential']\n",
    "# Learning rate\n",
    "learning_rate = [1, 0.5, 0.25, 0.1, 0.05, 0.01]\n",
    "# No of estimators\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64]\n",
    "# Max features\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "# No of levels of trees\n",
    "max_depths = [2,5,10,15]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Critetion \n",
    "criterion = ['friedman mse','mse','mae']\n",
    "\n",
    "# Parameters in the Grid for Randomized Search CV\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "                'learning_rate':learning_rate,\n",
    "                'criterion':criterion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 285 out of 300 | elapsed:   21.0s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   30.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=GradientBoostingClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['friedman mse', 'mse',\n",
       "                                                      'mae'],\n",
       "                                        'learning_rate': [1, 0.5, 0.25, 0.1,\n",
       "                                                          0.05, 0.01],\n",
       "                                        'max_depth': [2, 5, 10, 15],\n",
       "                                        'max_features': ['auto', 'sqrt',\n",
       "                                                         'log2'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [1, 2, 4, 8, 16, 32,\n",
       "                                                         64]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "gb_random = RandomizedSearchCV(estimator = gb, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "gb_random.fit(X_runs,y_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 32,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 10,\n",
       " 'learning_rate': 0.25,\n",
       " 'criterion': 'mae'}"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "gb_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting, training and testing the model again after Pruning\n",
    "gb_prune = GradientBoostingClassifier(criterion='mae',n_estimators=32,min_samples_split=4,min_samples_leaf=5,max_features='auto',max_depth=10,learning_rate=0.25)\n",
    "\n",
    "gb_prune.fit(X_train_runs,y_train_runs)\n",
    "\n",
    "gb_prune_pred_runs = gb_prune.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score after Pruning: 0.9856459330143541\n",
      "Precision Score after Pruning: 1.0\n",
      "Recall Score after Pruning: 0.9852941176470589\n",
      "F1 Score after Pruning: 0.9925925925925926\n",
      "ROC AUC Score after Pruning: 0.9926470588235294\n",
      "Train Score for Gradient Boosting Classifier after Pruning 0.9928909952606635\n",
      "Test Score for Gradient Boosting Classifier after Pruning 0.9856459330143541\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score after Pruning\n",
    "gb_prune_runs_accuracy = metrics.accuracy_score(gb_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score after Pruning\n",
    "print('Accuracy Score after Pruning:',gb_prune_runs_accuracy)\n",
    "\n",
    "# Precision Score after Pruning\n",
    "gb_prune_runs_precision = metrics.precision_score(gb_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Precision Score after Pruning\n",
    "print('Precision Score after Pruning:',gb_prune_runs_precision)\n",
    "\n",
    "# Recall Score after Pruning\n",
    "gb_prune_runs_recall = metrics.recall_score(gb_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Recall Score after Pruning\n",
    "print('Recall Score after Pruning:',gb_prune_runs_recall)\n",
    "\n",
    "# F1 Score after Pruning\n",
    "gb_prune_runs_f1score = metrics.f1_score(gb_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing F1 Score after Pruning\n",
    "print('F1 Score after Pruning:',gb_prune_runs_f1score)\n",
    "\n",
    "# ROC AUC Score after Pruning\n",
    "gb_prune_runs_aucrocscore = metrics.roc_auc_score(gb_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score after Pruning \n",
    "print('ROC AUC Score after Pruning:',gb_prune_runs_aucrocscore)\n",
    "\n",
    "# Train Score for Gradient Boosting Classifier after Pruning\n",
    "gb_prune_runs_train = gb_prune.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score for Gradient Boosting Classifier \n",
    "print('Train Score for Gradient Boosting Classifier after Pruning',gb_prune_runs_train)\n",
    "\n",
    "# Test Score for Gradient Boosting Classifier after Pruning\n",
    "gb_prune_runs_test = gb_prune.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score for Gradient Boosting Classifier after Pruning\n",
    "print('Test Score for Gradient Boosting Classifier after Pruning',gb_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we could see we have corrected Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7 Support Vector Machine Classifier <a id='svmr'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Support Vector Machine Classifier and fitting,training and testing the model\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "\n",
    "svc.fit(X_train_runs,y_train_runs)\n",
    "svc_pred_runs = svc.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9760765550239234\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.9757281553398058\n",
      "F1 Score: 0.9877149877149877\n",
      "ROC AUC Score: 0.9878640776699029\n",
      "Train Score for Support Machine Classifier 0.990521327014218\n",
      "Test Score for Gradient Boosting Classifier 0.9760765550239234\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "svc_runs_accuracy = metrics.accuracy_score(svc_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',svc_runs_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "svc_runs_precision = metrics.precision_score(svc_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',svc_runs_precision)\n",
    "\n",
    "# Recall Score\n",
    "svc_runs_recall = metrics.recall_score(svc_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',svc_runs_recall)\n",
    "\n",
    "# F1 Score\n",
    "svc_runs_f1score = metrics.f1_score(svc_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',svc_runs_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "svc_runs_aucrocscore = metrics.roc_auc_score(svc_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',svc_runs_aucrocscore)\n",
    "\n",
    "# Train Score for Support Machine Classifier\n",
    "svc_runs_train = svc.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score for Support Machine Classifier\n",
    "print('Train Score for Support Machine Classifier',svc_runs_train)\n",
    "\n",
    "# Test Score for Support Machine Classifier\n",
    "svc_runs_test = svc.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score for Support Machine Classifier\n",
    "print('Test Score for Gradient Boosting Classifier',svc_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine draws a hyperplane to reduce overfitting unlike other algorithms, and here too Precision is 1.0 through which we can tell there are no False Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8 KNearest Neighbors Classifier<a id='knnr'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the KNearest Neighbors model and fitting,training and testing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train_runs,y_train_runs)\n",
    "knn_pred_runs = knn.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9760765550239234\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.9757281553398058\n",
      "F1 Score: 0.9877149877149877\n",
      "ROC AUC Score: 0.9878640776699029\n",
      "Train Score for Support Machine Classifier 0.990521327014218\n",
      "Test Score for Gradient Boosting Classifier 0.9760765550239234\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "knn_runs_accuracy = metrics.accuracy_score(knn_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',knn_runs_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "knn_runs_precision = metrics.precision_score(knn_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',knn_runs_precision)\n",
    "\n",
    "# Recall Score\n",
    "knn_runs_recall = metrics.recall_score(knn_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',knn_runs_recall)\n",
    "\n",
    "# F1 Score\n",
    "knn_runs_f1score = metrics.f1_score(knn_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',knn_runs_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "knn_runs_aucrocscore = metrics.roc_auc_score(knn_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',knn_runs_aucrocscore)\n",
    "\n",
    "# Train Score for KNearest Neighbors Classifier\n",
    "knn_runs_train = knn.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score for KNearest Neighbors Classifier\n",
    "print('Train Score for Support Machine Classifier',knn_runs_train)\n",
    "\n",
    "# Test Score for KNearest Neighbors Classifier\n",
    "knn_runs_test = knn.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score for KNearest Neighbors Classifier\n",
    "print('Test Score for Gradient Boosting Classifier',knn_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As KNN works with distance criteria, this algorithm too prevents Overfitting, and here too there are no False Positives as Precision is 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.9 Bagging Classifier <a id='bagr'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Bagging Classifier and fitting,training and testing\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bc = BaggingClassifier()\n",
    "\n",
    "bc.fit(X_train_runs,y_train_runs)\n",
    "bc_pred_runs = bc.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9856459330143541\n",
      "Precision Score: 0.9950248756218906\n",
      "Recall Score: 0.9900990099009901\n",
      "F1 Score: 0.9925558312655087\n",
      "ROC AUC Score: 0.9236209335219236\n",
      "Train Score for Bagging Classifier 0.9976303317535545\n",
      "Test Score for Bagging Classifier 0.9856459330143541\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "bc_runs_accuracy = metrics.accuracy_score(bc_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',bc_runs_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "bc_runs_precision = metrics.precision_score(bc_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',bc_runs_precision)\n",
    "\n",
    "# Recall Score\n",
    "bc_runs_recall = metrics.recall_score(bc_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',bc_runs_recall)\n",
    "\n",
    "# F1 Score\n",
    "bc_runs_f1score = metrics.f1_score(bc_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',bc_runs_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "bc_runs_aucrocscore = metrics.roc_auc_score(bc_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',bc_runs_aucrocscore)\n",
    "\n",
    "# Train Score for Bagging Classifier\n",
    "bc_runs_train = bc.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score for Bagging Classifier\n",
    "print('Train Score for Bagging Classifier',bc_runs_train)\n",
    "\n",
    "# Test Score for Bagging Classifier\n",
    "bc_runs_test = bc.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score for Bagging Classifier\n",
    "print('Test Score for Bagging Classifier',bc_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Bagging Classifier reduces variance it thereby prevents Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.10 XG Boost Classifier <a id='xgbr'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing XG Boost Classifier and fitting,training and testing\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb.fit(X_train_runs,y_train_runs)\n",
    "xgb_pred_runs = xgb.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9808612440191388\n",
      "Precision Score: 0.9950248756218906\n",
      "Recall Score: 0.9852216748768473\n",
      "F1 Score: 0.9900990099009901\n",
      "ROC AUC Score: 0.9092775041050905\n",
      "Train Score for XG Boost Classifier 1.0\n",
      "Test Score for XG Boost Classifier 0.9808612440191388\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "xgb_runs_accuracy = metrics.accuracy_score(xgb_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',xgb_runs_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "xgb_runs_precision = metrics.precision_score(xgb_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',xgb_runs_precision)\n",
    "\n",
    "# Recall Score\n",
    "xgb_runs_recall = metrics.recall_score(xgb_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',xgb_runs_recall)\n",
    "\n",
    "# F1 Score\n",
    "xgb_runs_f1score = metrics.f1_score(xgb_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',xgb_runs_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "xgb_runs_aucrocscore = metrics.roc_auc_score(xgb_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',xgb_runs_aucrocscore)\n",
    "\n",
    "# Train Score for XG Boost Classifier\n",
    "xgb_runs_train = xgb.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score for XG Boost Classifier\n",
    "print('Train Score for XG Boost Classifier',xgb_runs_train)\n",
    "\n",
    "# Test Score for XG Boost Classifier\n",
    "xgb_runs_test = xgb.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score for XG Boost Classifier\n",
    "print('Test Score for XG Boost Classifier',xgb_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prevent the Overfitting through Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.11 Pruned XG Boost Classifier<a id='xgbrprune'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning Grid for Randomized Search CV\n",
    "params_xgb_GS = {\"max_depth\": [3,5,6,7,8],\n",
    "              \"min_child_weight\" : [5,6,7,8],\n",
    "            'learning_rate':[0.05,0.1,0.2],\n",
    "            'n_estimators': [10,30,50,70]}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "xgb_random = RandomizedSearchCV(estimator = xgb, param_distributions = params_xgb_GS, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 285 out of 300 | elapsed:    2.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                           colsample_bylevel=1,\n",
       "                                           colsample_bynode=1,\n",
       "                                           colsample_bytree=1, gamma=0,\n",
       "                                           gpu_id=-1, importance_type='gain',\n",
       "                                           interaction_constraints='',\n",
       "                                           learning_rate=0.300000012,\n",
       "                                           max_delta_step=0, max_depth=6,\n",
       "                                           min_child_weight=1, missing=nan,\n",
       "                                           monotone_constraints='()',\n",
       "                                           n_estimators=100, n_jobs=0,\n",
       "                                           num_parallel_tree=1, random_state=0,\n",
       "                                           reg_alpha=0, reg_lambda=1,\n",
       "                                           scale_pos_weight=1, subsample=1,\n",
       "                                           tree_method='exact',\n",
       "                                           validate_parameters=1,\n",
       "                                           verbosity=None),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'learning_rate': [0.05, 0.1, 0.2],\n",
       "                                        'max_depth': [3, 5, 6, 7, 8],\n",
       "                                        'min_child_weight': [5, 6, 7, 8],\n",
       "                                        'n_estimators': [10, 30, 50, 70]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the random search model\n",
    "xgb_random.fit(X_runs,y_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 50,\n",
       " 'min_child_weight': 5,\n",
       " 'max_depth': 6,\n",
       " 'learning_rate': 0.1}"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "xgb_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting,training and testing the model after Pruning\n",
    "xgb_prune = XGBClassifier(n_estimators=50,min_child_weight=5,max_depth=6,learning_rate=0.1)\n",
    "\n",
    "xgb_prune.fit(X_train_runs,y_train_runs)\n",
    "\n",
    "xgb_prune_pred_runs = xgb_prune.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score after Pruning: 0.9808612440191388\n",
      "Precision Score after Pruning: 0.9950248756218906\n",
      "Recall Score after Pruning: 0.9852216748768473\n",
      "F1 Score after Pruning: 0.9900990099009901\n",
      "ROC AUC Score after Pruning: 0.9092775041050905\n",
      "Train Score for XG Boost Classifier after Pruning 0.990521327014218\n",
      "Test Score for XG Boost Classifier after Pruning 0.9808612440191388\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score after Pruning\n",
    "xgb_prune_runs_accuracy = metrics.accuracy_score(xgb_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score after Pruning\n",
    "print('Accuracy Score after Pruning:',xgb_prune_runs_accuracy)\n",
    "\n",
    "# Precision Score after Pruning\n",
    "xgb_prune_runs_precision = metrics.precision_score(xgb_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Precision Score after Pruning\n",
    "print('Precision Score after Pruning:',xgb_prune_runs_precision)\n",
    "\n",
    "# Recall Score after Pruning\n",
    "xgb_prune_runs_recall = metrics.recall_score(xgb_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Recall Score after Pruning\n",
    "print('Recall Score after Pruning:',xgb_prune_runs_recall)\n",
    "\n",
    "# F1 Score after Pruning\n",
    "xgb_prune_runs_f1score = metrics.f1_score(xgb_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing F1 Score after Pruning\n",
    "print('F1 Score after Pruning:',xgb_prune_runs_f1score)\n",
    "\n",
    "# ROC AUC Score after Pruning\n",
    "xgb_prune_runs_aucrocscore = metrics.roc_auc_score(xgb_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score after Pruning\n",
    "print('ROC AUC Score after Pruning:',xgb_prune_runs_aucrocscore)\n",
    "\n",
    "# Train Score for XG Boost Classifier after Pruning\n",
    "xgb_prune_runs_train = xgb_prune.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score for XG Boost Classifier after Pruning\n",
    "print('Train Score for XG Boost Classifier after Pruning',xgb_prune_runs_train)\n",
    "\n",
    "# Test Score for XG Boost Classifier after Pruning\n",
    "xgb_prune_runs_test = xgb_prune.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score for XG Boost Classifier after Pruning\n",
    "print('Test Score for XG Boost Classifier after Pruning',xgb_prune_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have corrected Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.12 Ada Boost Classifier <a id='abr'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Ada Boost Classifier and fitting,training and testing the model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ab = AdaBoostClassifier()\n",
    "\n",
    "ab.fit(X_train_runs,y_train_runs)\n",
    "\n",
    "ab_pred_runs = ab.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9856459330143541\n",
      "Precision Score: 0.9950248756218906\n",
      "Recall Score: 0.9900990099009901\n",
      "F1 Score: 0.9925558312655087\n",
      "ROC AUC Score: 0.9236209335219236\n",
      "Train Score for Ada Boost Classifier 1.0\n",
      "Test Score for Ada Boost Classifier 0.9856459330143541\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "ab_runs_accuracy = metrics.accuracy_score(ab_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',ab_runs_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "ab_runs_precision = metrics.precision_score(ab_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',ab_runs_precision)\n",
    "\n",
    "# Recall Score\n",
    "ab_runs_recall = metrics.recall_score(ab_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',ab_runs_recall)\n",
    "\n",
    "# F1 Score\n",
    "ab_runs_f1score = metrics.f1_score(ab_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',ab_runs_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "ab_runs_aucrocscore = metrics.roc_auc_score(ab_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',ab_runs_aucrocscore)\n",
    "\n",
    "# Train Score for Ada Boost Classifier\n",
    "ab_runs_train = ab.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score for Ada Boost Classifier\n",
    "print('Train Score for Ada Boost Classifier',ab_runs_train)\n",
    "\n",
    "# Test Score for Ada Boost Classifier\n",
    "ab_runs_test = ab.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score for Ada Boost Classifier\n",
    "print('Test Score for Ada Boost Classifier',ab_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Overfitting should be corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.13 Pruned Ada Boost Classifier <a id='abrprune'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the parameters to a Grid for performing Randomized Search CV\n",
    "params_Adb_GS = {'learning_rate':[0.05,0.1,0.2,1],'n_estimators':[10,30,50,60,70,75],'algorithm':['SAMME', 'SAMME.R']}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "ab_random = RandomizedSearchCV(estimator = ab,param_distributions=params_Adb_GS,n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:282: UserWarning: The total space of parameters 48 is smaller than n_iter=100. Running 48 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed:    4.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=AdaBoostClassifier(), n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'algorithm': ['SAMME', 'SAMME.R'],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.2, 1],\n",
       "                                        'n_estimators': [10, 30, 50, 60, 70,\n",
       "                                                         75]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the Randomized Search CV\n",
    "ab_random.fit(X_runs,y_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 10, 'learning_rate': 0.2, 'algorithm': 'SAMME'}"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "ab_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting,training and testing the model after Pruning\n",
    "ab_prune = AdaBoostClassifier(n_estimators=10,learning_rate=0.2,algorithm='SAMME')\n",
    "\n",
    "ab_prune.fit(X_train_runs,y_train_runs)\n",
    "\n",
    "ab_prune_pred_runs = ab_prune.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score after Pruning: 0.9808612440191388\n",
      "Precision Score after Pruning: 0.9950248756218906\n",
      "Recall Score after Pruning: 0.9852216748768473\n",
      "F1 Score after Pruning: 0.9900990099009901\n",
      "ROC AUC Score after Pruning: 0.9092775041050905\n",
      "Train Score for XG Boost Classifier after Pruning 0.995260663507109\n",
      "Test Score for XG Boost Classifier after Pruning 0.9808612440191388\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score after Pruning\n",
    "ab_prune_runs_accuracy = metrics.accuracy_score(ab_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score after Pruning\n",
    "print('Accuracy Score after Pruning:',ab_prune_runs_accuracy)\n",
    "\n",
    "# Precision Score after Pruning\n",
    "ab_prune_runs_precision = metrics.precision_score(ab_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Precision Score after Pruning\n",
    "print('Precision Score after Pruning:',ab_prune_runs_precision)\n",
    "\n",
    "# Recall Score after Pruning\n",
    "ab_prune_runs_recall = metrics.recall_score(ab_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Recall Score after Pruning\n",
    "print('Recall Score after Pruning:',ab_prune_runs_recall)\n",
    "\n",
    "# F1 Score after Pruning\n",
    "ab_prune_runs_f1score = metrics.f1_score(ab_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing F1 Score after Pruning\n",
    "print('F1 Score after Pruning:',ab_prune_runs_f1score)\n",
    "\n",
    "# ROC AUC Score after Pruning\n",
    "ab_prune_runs_aucrocscore = metrics.roc_auc_score(ab_prune_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score after Pruning\n",
    "print('ROC AUC Score after Pruning:',ab_prune_runs_aucrocscore)\n",
    "\n",
    "# Train Score for Ada Boost Classifier after Pruning\n",
    "ab_prune_runs_train = ab_prune.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score for Ada Boost Classifier after Pruning\n",
    "print('Train Score for Ada Boost Classifier after Pruning',ab_prune_runs_train)\n",
    "\n",
    "# Test Score for Ada Boost Classifier after Pruning\n",
    "ab_prune_runs_test = ab_prune.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score for Ada Boost Classifier after Pruning\n",
    "print('Test Score for Ada Boost Classifier after Pruning',ab_prune_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prevented Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.14 Logistic Regression <a id='lrr'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Importing Logistic Regression and fitting,training and testing the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train_runs,y_train_runs)\n",
    "\n",
    "lr_pred_runs = lr.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9808612440191388\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.9804878048780488\n",
      "F1 Score: 0.9901477832512315\n",
      "ROC AUC Score: 0.9902439024390244\n",
      "Train Score for Logistic Regression 1.0\n",
      "Test Score for Logistic Regression 0.9808612440191388\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "lr_runs_accuracy = metrics.accuracy_score(lr_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',lr_runs_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "lr_runs_precision = metrics.precision_score(lr_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',lr_runs_precision)\n",
    "\n",
    "# Recall Score\n",
    "lr_runs_recall = metrics.recall_score(lr_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',lr_runs_recall)\n",
    "\n",
    "# F1 Score\n",
    "lr_runs_f1score = metrics.f1_score(lr_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',lr_runs_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "lr_runs_aucrocscore = metrics.roc_auc_score(lr_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',lr_runs_aucrocscore)\n",
    "\n",
    "# Train Score for Logistic Regression\n",
    "lr_runs_train = lr.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score for Logistic Regression\n",
    "print('Train Score for Logistic Regression',lr_runs_train)\n",
    "\n",
    "# Test Score for Logistic Regression\n",
    "lr_runs_test = lr.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score for Logistic Regression\n",
    "print('Test Score for Logistic Regression',lr_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see Overfitting, as Logistic Regression is a base model lets try using it as a meta classifier for Stacking to stack other models into it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.15 Stacking for Logistic Regression using Mlxtend Classifier <a id='strmlx'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Mlxtend classifier and fitting,training and testing\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "# Stacking KNN,SVM,XG Boost,Ada Boost and Gradient Boosting models into Logistic Regression for better results\n",
    "lrstack = StackingClassifier(classifiers=[knn,svc,xgb_prune,ab_prune,gb_prune],meta_classifier=lr)\n",
    "\n",
    "lrstack.fit(X_train_runs,y_train_runs)\n",
    "\n",
    "lrstack_pred_runs = st.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score after Stacking for Logistic Regression: 0.9760765550239234\n",
      "Precision Score after Stacking for Logistic Regression: 1.0\n",
      "Recall Score after Stacking for Logistic Regression: 0.9757281553398058\n",
      "F1 Score after Stacking for Logistic Regression: 0.9877149877149877\n",
      "ROC AUC Score after Stacking for Logistic Regression: 0.9878640776699029\n",
      "Train Score for Logistic Regression after Stacking for Logistic Regression 0.9928909952606635\n",
      "Test Score for Logistic Regression after Stacking for Logistic Regression 0.9760765550239234\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score after Stacking for Logistic Regression\n",
    "lrstack_runs_accuracy = metrics.accuracy_score(lrstack_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score after Stacking for Logistic Regression\n",
    "print('Accuracy Score after Stacking for Logistic Regression:',lrstack_runs_accuracy)\n",
    "\n",
    "# Precision Score after Stacking for Logistic Regression\n",
    "lrstack_runs_precision = metrics.precision_score(lrstack_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Precision Score after Stacking for Logistic Regression\n",
    "print('Precision Score after Stacking for Logistic Regression:',lrstack_runs_precision)\n",
    "\n",
    "# Recall Score after Stacking for Logistic Regression\n",
    "lrstack_runs_recall = metrics.recall_score(lrstack_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Recall Score after Stacking for Logistic Regression\n",
    "print('Recall Score after Stacking for Logistic Regression:',lrstack_runs_recall)\n",
    "\n",
    "# F1 Score after Stacking for Logistic Regression\n",
    "lrstack_runs_f1score = metrics.f1_score(lrstack_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing F1 Score after Stacking for Logistic Regression\n",
    "print('F1 Score after Stacking for Logistic Regression:',lrstack_runs_f1score)\n",
    "\n",
    "# ROC AUC Score after Stacking for Logistic Regression\n",
    "lrstack_runs_aucrocscore = metrics.roc_auc_score(lrstack_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score after Stacking for Logistic Regression\n",
    "print('ROC AUC Score after Stacking for Logistic Regression:',lrstack_runs_aucrocscore)\n",
    "\n",
    "# Train Score for Logistic Regression after Stacking for Logistic Regression\n",
    "lrstack_runs_train = lrstack.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score for Logistic Regression after Stacking for Logistic Regression\n",
    "print('Train Score for Logistic Regression after Stacking for Logistic Regression',lrstack_runs_train)\n",
    "\n",
    "# Test Score for Logistic Regression after Stacking for Logistic Regression\n",
    "lrstack_runs_test = lrstack.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score for Logistic Regression after Stacking for Logistic Regression\n",
    "print('Test Score for Logistic Regression after Stacking for Logistic Regression',lrstack_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Logistic Regression give better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.16 Stacking using Voting Classifier <a id='strvote'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning estimator models for voting classifier\n",
    "vote_est = [('knn',knn),('xgb',xgb_prune),('SVM',svc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Voting Classifier and fitting the model\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "vote = VotingClassifier(estimators=vote_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing the model\n",
    "vote.fit(X_train_runs,y_train_runs)\n",
    "\n",
    "vote_pred_runs = vote.predict(X_test_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9760765550239234\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.9757281553398058\n",
      "F1 Score: 0.9877149877149877\n",
      "ROC AUC Score: 0.9878640776699029\n",
      "Train Score 0.990521327014218\n",
      "Test Score 0.9760765550239234\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "vote_runs_accuracy = metrics.accuracy_score(vote_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',vote_runs_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "vote_runs_precision = metrics.precision_score(vote_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',vote_runs_precision)\n",
    "\n",
    "# Recall Score\n",
    "vote_runs_recall = metrics.recall_score(vote_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',vote_runs_recall)\n",
    "\n",
    "# F1 Score\n",
    "vote_runs_f1score = metrics.f1_score(vote_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',vote_runs_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "vote_runs_aucrocscore = metrics.roc_auc_score(vote_pred_runs,y_test_runs)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',vote_runs_aucrocscore)\n",
    "\n",
    "# Train Score\n",
    "vote_runs_train = vote.score(X_train_runs,y_train_runs)\n",
    "\n",
    "# Printing Train Score\n",
    "print('Train Score',vote_runs_train)\n",
    "\n",
    "# Test Score\n",
    "vote_runs_test = vote.score(X_test_runs,y_test_runs)\n",
    "\n",
    "# Printing Test Score\n",
    "print('Test Score',vote_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.17 Comparison Table of all models for Runs Prediction <a id='ctr'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary with all the metrics\n",
    "runs_metrics = {'Classifier': ['Decision Tree','Pruned Decision Tree','Random Forest','Pruned Random Forest','Gradient Boosting','Pruned Gradient Boosting','Support Vector Machine','KNN','Bagging','XG Boost','Pruned XG Boost','Ada Boost','Pruned Ada Boost','Logistic Regression','Stacking for Logistic Regression using Mlxtend','Stacking using Voting'],\n",
    "                'Accuracy':[dt_runs_accuracy,dt_prune_runs_accuracy,rf_runs_accuracy,rf_prune_runs_accuracy,gb_runs_accuracy,gb_prune_runs_accuracy,svc_runs_accuracy,knn_runs_accuracy,bc_runs_accuracy,xgb_runs_accuracy,xgb_prune_runs_accuracy,ab_runs_accuracy,ab_prune_runs_accuracy,lr_runs_accuracy,lrstack_runs_accuracy,vote_runs_accuracy],\n",
    "                'Precision':[dt_runs_precision,dt_prune_runs_precision,rf_runs_precision,rf_prune_runs_precision,gb_runs_precision,gb_prune_runs_precision,svc_runs_precision,knn_runs_precision,bc_runs_precision,xgb_runs_precision,xgb_prune_runs_precision,ab_runs_precision,ab_prune_runs_precision,lr_runs_precision,lrstack_runs_precision,vote_runs_precision],\n",
    "                'Recall':[dt_runs_recall,dt_prune_runs_recall,rf_runs_recall,rf_prune_runs_recall,gb_runs_recall,gb_prune_runs_recall,svc_runs_recall,knn_runs_recall,bc_runs_recall,xgb_runs_recall,xgb_prune_runs_recall,ab_runs_recall,ab_prune_runs_recall,lr_runs_recall,lrstack_runs_recall,vote_runs_recall],\n",
    "                'F1 Score':[dt_runs_f1score,dt_prune_runs_f1score,rf_runs_f1score,rf_prune_runs_f1score,gb_runs_f1score,gb_prune_runs_f1score,svc_runs_f1score,knn_runs_f1score,bc_runs_f1score,xgb_runs_f1score,xgb_prune_runs_f1score,ab_runs_f1score,ab_prune_runs_f1score,lr_runs_f1score,lrstack_runs_f1score,vote_runs_f1score],\n",
    "                'AUCROC Score':[dt_runs_aucrocscore,dt_prune_runs_aucrocscore,rf_runs_aucrocscore,rf_prune_runs_aucrocscore,gb_runs_aucrocscore,gb_prune_runs_aucrocscore,svc_runs_aucrocscore,knn_runs_aucrocscore,bc_runs_aucrocscore,xgb_runs_aucrocscore,xgb_prune_runs_aucrocscore,ab_runs_aucrocscore,ab_prune_runs_aucrocscore,lr_runs_aucrocscore,lrstack_runs_aucrocscore,vote_runs_aucrocscore],\n",
    "                'Train Score':[dt_runs_train,dt_prune_runs_train,rf_runs_train,rf_prune_runs_train,gb_runs_train,gb_prune_runs_train,svc_runs_train,knn_runs_train,xgb_runs_train,bc_runs_train,xgb_prune_runs_train,ab_runs_train,ab_prune_runs_train,lr_runs_train,lrstack_runs_train,vote_runs_train],\n",
    "                'Test Score':[dt_runs_test,dt_prune_runs_test,rf_runs_test,rf_prune_runs_test,gb_runs_test,gb_prune_runs_test,svc_runs_test,knn_runs_test,xgb_runs_test,bc_runs_test,xgb_prune_runs_test,ab_runs_test,ab_prune_runs_test,lr_runs_test,lrstack_runs_test,vote_runs_test]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the dictionary as Dataframe\n",
    "runs_metrics = pd.DataFrame(runs_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>AUCROC Score</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.961722</td>\n",
       "      <td>0.990050</td>\n",
       "      <td>0.970732</td>\n",
       "      <td>0.980296</td>\n",
       "      <td>0.735366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pruned Decision Tree</td>\n",
       "      <td>0.961722</td>\n",
       "      <td>0.990050</td>\n",
       "      <td>0.970732</td>\n",
       "      <td>0.980296</td>\n",
       "      <td>0.735366</td>\n",
       "      <td>0.997630</td>\n",
       "      <td>0.961722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.976077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975728</td>\n",
       "      <td>0.987715</td>\n",
       "      <td>0.987864</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pruned Random Forest</td>\n",
       "      <td>0.985646</td>\n",
       "      <td>0.990050</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.992519</td>\n",
       "      <td>0.886389</td>\n",
       "      <td>0.990521</td>\n",
       "      <td>0.985646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.985646</td>\n",
       "      <td>0.995025</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.992556</td>\n",
       "      <td>0.923621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pruned Gradient Boosting</td>\n",
       "      <td>0.985646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985294</td>\n",
       "      <td>0.992593</td>\n",
       "      <td>0.992647</td>\n",
       "      <td>0.992891</td>\n",
       "      <td>0.985646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.976077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975728</td>\n",
       "      <td>0.987715</td>\n",
       "      <td>0.987864</td>\n",
       "      <td>0.990521</td>\n",
       "      <td>0.976077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.976077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975728</td>\n",
       "      <td>0.987715</td>\n",
       "      <td>0.987864</td>\n",
       "      <td>0.990521</td>\n",
       "      <td>0.976077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>0.985646</td>\n",
       "      <td>0.995025</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.992556</td>\n",
       "      <td>0.923621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XG Boost</td>\n",
       "      <td>0.980861</td>\n",
       "      <td>0.995025</td>\n",
       "      <td>0.985222</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.909278</td>\n",
       "      <td>0.997630</td>\n",
       "      <td>0.985646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pruned XG Boost</td>\n",
       "      <td>0.980861</td>\n",
       "      <td>0.995025</td>\n",
       "      <td>0.985222</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.909278</td>\n",
       "      <td>0.990521</td>\n",
       "      <td>0.980861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ada Boost</td>\n",
       "      <td>0.985646</td>\n",
       "      <td>0.995025</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.992556</td>\n",
       "      <td>0.923621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Pruned Ada Boost</td>\n",
       "      <td>0.980861</td>\n",
       "      <td>0.995025</td>\n",
       "      <td>0.985222</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.909278</td>\n",
       "      <td>0.995261</td>\n",
       "      <td>0.980861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.980861</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980488</td>\n",
       "      <td>0.990148</td>\n",
       "      <td>0.990244</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Stacking for Logistic Regression using Mlxtend</td>\n",
       "      <td>0.976077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975728</td>\n",
       "      <td>0.987715</td>\n",
       "      <td>0.987864</td>\n",
       "      <td>0.992891</td>\n",
       "      <td>0.976077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Stacking using Voting</td>\n",
       "      <td>0.976077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975728</td>\n",
       "      <td>0.987715</td>\n",
       "      <td>0.987864</td>\n",
       "      <td>0.990521</td>\n",
       "      <td>0.976077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Classifier  Accuracy  Precision  \\\n",
       "0                                    Decision Tree  0.961722   0.990050   \n",
       "1                             Pruned Decision Tree  0.961722   0.990050   \n",
       "2                                    Random Forest  0.976077   1.000000   \n",
       "3                             Pruned Random Forest  0.985646   0.990050   \n",
       "4                                Gradient Boosting  0.985646   0.995025   \n",
       "5                         Pruned Gradient Boosting  0.985646   1.000000   \n",
       "6                           Support Vector Machine  0.976077   1.000000   \n",
       "7                                              KNN  0.976077   1.000000   \n",
       "8                                          Bagging  0.985646   0.995025   \n",
       "9                                         XG Boost  0.980861   0.995025   \n",
       "10                                 Pruned XG Boost  0.980861   0.995025   \n",
       "11                                       Ada Boost  0.985646   0.995025   \n",
       "12                                Pruned Ada Boost  0.980861   0.995025   \n",
       "13                             Logistic Regression  0.980861   1.000000   \n",
       "14  Stacking for Logistic Regression using Mlxtend  0.976077   1.000000   \n",
       "15                           Stacking using Voting  0.976077   1.000000   \n",
       "\n",
       "      Recall  F1 Score  AUCROC Score  Train Score  Test Score  \n",
       "0   0.970732  0.980296      0.735366     1.000000    0.961722  \n",
       "1   0.970732  0.980296      0.735366     0.997630    0.961722  \n",
       "2   0.975728  0.987715      0.987864     1.000000    0.976077  \n",
       "3   0.995000  0.992519      0.886389     0.990521    0.985646  \n",
       "4   0.990099  0.992556      0.923621     1.000000    0.985646  \n",
       "5   0.985294  0.992593      0.992647     0.992891    0.985646  \n",
       "6   0.975728  0.987715      0.987864     0.990521    0.976077  \n",
       "7   0.975728  0.987715      0.987864     0.990521    0.976077  \n",
       "8   0.990099  0.992556      0.923621     1.000000    0.980861  \n",
       "9   0.985222  0.990099      0.909278     0.997630    0.985646  \n",
       "10  0.985222  0.990099      0.909278     0.990521    0.980861  \n",
       "11  0.990099  0.992556      0.923621     1.000000    0.985646  \n",
       "12  0.985222  0.990099      0.909278     0.995261    0.980861  \n",
       "13  0.980488  0.990148      0.990244     1.000000    0.980861  \n",
       "14  0.975728  0.987715      0.987864     0.992891    0.976077  \n",
       "15  0.975728  0.987715      0.987864     0.990521    0.976077  "
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the dataframe\n",
    "runs_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among all the models we could see Pruned XG Boost,Pruned Gradient Boosting are best models with least Train and Test score difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Splitting data for Wickets prediction <a id='wsplit'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning X and y for train test split\n",
    "X_wickets = cricket.drop('Wickets',1)\n",
    "y_wickets = cricket['Wickets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and test using train test split\n",
    "X_train_wickets,X_test_wickets,y_train_wickets,y_test_wickets = train_test_split(X_wickets,y_wickets,test_size=0.33,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Models for Wicket Predictions <a id ='wm'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Decision Tree <a id='dtw'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting,training and testing the model\n",
    "\n",
    "dtw = DecisionTreeClassifier(class_weight='balanced')\n",
    "\n",
    "dtw.fit(X_train_wickets,y_train_wickets)\n",
    "\n",
    "dt_pred_wickets = dtw.predict(X_test_wickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9234449760765551\n",
      "Precision Score: 0.953125\n",
      "Recall Score: 0.9631578947368421\n",
      "F1 Score: 0.9581151832460733\n",
      "ROC AUC Score: 0.7447368421052631\n",
      "Train Score for Decision Tree Classifier 1.0\n",
      "Test Score for Decision Tree Classifier 0.9234449760765551\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "dt_wickets_accuracy = metrics.accuracy_score(dt_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',dt_wickets_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "dt_wickets_precision = metrics.precision_score(dt_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',dt_wickets_precision)\n",
    "\n",
    "# Recall Score\n",
    "dt_wickets_recall = metrics.recall_score(dt_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',dt_wickets_recall)\n",
    "\n",
    "# F1 Score\n",
    "dt_wickets_f1score = metrics.f1_score(dt_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',dt_wickets_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "dt_wickets_aucrocscore = metrics.roc_auc_score(dt_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',dt_wickets_aucrocscore)\n",
    "\n",
    "# Train Score for Decision Tree Classifier\n",
    "dt_wickets_train = dtw.score(X_train_wickets,y_train_wickets)\n",
    "\n",
    "# Printing Train Score for Decision Tree Classifier\n",
    "print('Train Score for Decision Tree Classifier',dt_wickets_train)\n",
    "\n",
    "# Test Score for Decision Tree Classifier\n",
    "dt_wickets_test = dtw.score(X_test_runs,y_test_wickets)\n",
    "\n",
    "# Printing Test Score for Decision Tree Classifier\n",
    "print('Test Score for Decision Tree Classifier',dt_wickets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that the scores are good but as the Train score is 1.0 we can confirm that the model is Overfitting\n",
    "\n",
    "To eliminate Overfitting we need to perform Pruning Techniques (ie) tune the parameters of the model\n",
    "\n",
    "The tuning of the parameters are done by Randomized Search CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Pruned Decision Tree Classifier<a id ='dtwprune'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion for tree\n",
    "criterion = ['gini','entropy']\n",
    "# Splitter for tree\n",
    "splitter = ['random','best']\n",
    "# Maximum levels of  trees\n",
    "max_depth = [2,5,10,15]\n",
    "# Maximum number of samples required to split a node\n",
    "max_leaf_nodes = [2, 5, 10]\n",
    "# Maximum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "\n",
    "# Parameter grid assigned for performing Randomized Sear\n",
    "random_grid = {'criterion': criterion,\n",
    "               'splitter': splitter,\n",
    "               'max_depth': max_depth,\n",
    "               'max_leaf_nodes': max_leaf_nodes,\n",
    "               'min_samples_leaf': min_samples_leaf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=DecisionTreeClassifier(class_weight='balanced'),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [2, 5, 10, 15],\n",
       "                                        'max_leaf_nodes': [2, 5, 10],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'splitter': ['random', 'best']},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 820,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "dt_random = RandomizedSearchCV(estimator = dtw, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "dt_random.fit(X_wickets,y_wickets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'splitter': 'random',\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_leaf_nodes': 10,\n",
       " 'max_depth': 15,\n",
       " 'criterion': 'entropy'}"
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "dt_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model again after parameter tuning by assigning the parameters given by Randomized Search CV\n",
    "dtw_prune = DecisionTreeClassifier(class_weight='balanced',criterion = \"entropy\", splitter = 'random', max_leaf_nodes = 10, min_samples_leaf = 4,max_depth= 15)\n",
    "\n",
    "dtw_prune.fit(X_train_wickets,y_train_wickets)\n",
    "dt_prune_wickets_pred = dtw_prune.predict(X_test_wickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score after Pruning: 0.9138755980861244\n",
      "Precision Score after Pruning: 0.9270833333333334\n",
      "Recall Score after Pruning: 0.978021978021978\n",
      "F1 Score after Pruning: 0.9518716577540107\n",
      "ROC AUC Score after Pruning: 0.7297517297517297\n",
      "Train Score for Decision Tree Classifier after Pruning 0.9478672985781991\n",
      "Test Score for Decision Tree Classifier  0.9138755980861244\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score after Pruning\n",
    "dt_prune_wickets_accuracy = metrics.accuracy_score(dt_prune_wickets_pred,y_test_wickets)\n",
    "\n",
    "# Printing Accuracy Score after Pruning\n",
    "print('Accuracy Score after Pruning:',dt_prune_wickets_accuracy)\n",
    "\n",
    "# Precision Score after Pruning\n",
    "dt_prune_wickets_precision = metrics.precision_score(dt_prune_wickets_pred,y_test_wickets)\n",
    "\n",
    "# Printing Precision Score after Pruning\n",
    "print('Precision Score after Pruning:',dt_prune_wickets_precision)\n",
    "\n",
    "# Recall Score after Pruning\n",
    "dt_prune_wickets_recall = metrics.recall_score(dt_prune_wickets_pred,y_test_wickets)\n",
    "\n",
    "# Printing Recall Score after Pruning\n",
    "print('Recall Score after Pruning:',dt_prune_wickets_recall)\n",
    "\n",
    "# F1 Score after Pruning\n",
    "dt_prune_wickets_f1score = metrics.f1_score(dt_prune_wickets_pred,y_test_wickets)\n",
    "\n",
    "# Printing F1 Score after Pruning\n",
    "print('F1 Score after Pruning:',dt_prune_wickets_f1score)\n",
    "\n",
    "# ROC AUC Score after Pruning\n",
    "dt_prune_wickets_aucrocscore = metrics.roc_auc_score(dt_prune_wickets_pred,y_test_wickets)\n",
    "\n",
    "# Printing ROC AUC Score after Pruning\n",
    "print('ROC AUC Score after Pruning:',dt_prune_wickets_aucrocscore)\n",
    "\n",
    "# Train Score for Decision Tree Classifier after Pruning\n",
    "dt_prune_wickets_train = dtw_prune.score(X_train_wickets,y_train_wickets)\n",
    "\n",
    "# Printing Train Score for Decision Tree Classifier after Pruning\n",
    "print('Train Score for Decision Tree Classifier after Pruning',dt_prune_wickets_train)\n",
    "\n",
    "# Test Score for Decision Tree Classifier after Pruning\n",
    "dt_prune_wickets_test = dtw_prune.score(X_test_runs,y_test_wickets)\n",
    "\n",
    "# Printing Test Score for Decision Tree Classifier after Pruning\n",
    "print('Test Score for Decision Tree Classifier ',dt_prune_wickets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see after Pruning its not Overfitting that much like before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Random Forest Classifier <a id='rfw'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting, training and testing the model\n",
    "rfw = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "rfw.fit(X_train_runs,y_train_wickets)\n",
    "rf_pred_wickets = rfw.predict(X_test_wickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9186602870813397\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.9186602870813397\n",
      "F1 Score: 0.9576059850374066\n",
      "ROC AUC Score after Pruning: 0.8013136288998357\n",
      "Train Score for Random Forest Classifier 0.9478672985781991\n",
      "Test Score for Random Forest Classifier 0.9186602870813397\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "rf_wickets_accuracy = metrics.accuracy_score(rf_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',rf_wickets_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "rf_wickets_precision = metrics.precision_score(rf_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',rf_wickets_precision)\n",
    "\n",
    "# Recall Score\n",
    "rf_wickets_recall = metrics.recall_score(rf_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',rf_wickets_recall)\n",
    "\n",
    "# F1 Score\n",
    "rf_wickets_f1score = metrics.f1_score(rf_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',rf_wickets_f1score)\n",
    "\n",
    "# ROC AUC Score after Pruning\n",
    "rf_wickets_aucrocscore = metrics.roc_auc_score(rf_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing ROC AUC Score after Pruning\n",
    "print('ROC AUC Score after Pruning:',rf_wickets_aucrocscore)\n",
    "\n",
    "\n",
    "# Train Score for Random Forest Classifier\n",
    "rf_wickets_train = rfw.score(X_train_wickets,y_train_wickets)\n",
    "\n",
    "# Printing Train Score for Random Forest Classifier\n",
    "print('Train Score for Random Forest Classifier',rf_wickets_train)\n",
    "\n",
    "# Test Score for Random Forest Classifier\n",
    "rf_wickets_test = rfw.score(X_test_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Test Score for Random Forest Classifier\n",
    "print('Test Score for Random Forest Classifier',rf_wickets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see a bit of Overfitting here, and here Precision Score is 1.0 so there are no False Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 Gradient Boosting Classifier <a id='gbw'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting, training and testing the model\n",
    "gbw = GradientBoostingClassifier()\n",
    "\n",
    "gbw.fit(X_train_wickets,y_train_wickets)\n",
    "gb_pred_wickets = gbw.predict(X_test_wickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9569377990430622\n",
      "Precision Score: 0.9791666666666666\n",
      "Recall Score 0.9740932642487047\n",
      "F1 Score: 0.9766233766233766\n",
      "ROC AUC Score: 0.8620466321243524\n",
      "Train Score for Gradient Boosting Classifier 1.0\n",
      "Test Score for Gradient Boosting Classifier 0.9569377990430622\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "gb_wickets_accuracy = metrics.accuracy_score(gb_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Accuracy Score \n",
    "print('Accuracy Score:',gb_wickets_accuracy)\n",
    "\n",
    "# Precision Score \n",
    "gb_wickets_precision = metrics.precision_score(gb_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Precision Score \n",
    "print('Precision Score:',gb_wickets_precision)\n",
    "\n",
    "# Recall Score\n",
    "gb_wickets_recall = metrics.recall_score(gb_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Recall Score \n",
    "print('Recall Score',gb_wickets_recall)\n",
    "\n",
    "# F1 Score \n",
    "gb_wickets_f1score = metrics.f1_score(gb_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing F1 Score \n",
    "print('F1 Score:',gb_wickets_f1score)\n",
    "\n",
    "# ROC AUC Score \n",
    "gb_wickets_aucrocscore = metrics.roc_auc_score(gb_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing ROC AUC Score \n",
    "print('ROC AUC Score:',gb_wickets_aucrocscore)\n",
    "\n",
    "# Train Score for Gradient Boosting Classifier \n",
    "gb_wickets_train = gbw.score(X_train_wickets,y_train_wickets)\n",
    "\n",
    "# Printing Train Score for Gradient Boosting Classifier \n",
    "print('Train Score for Gradient Boosting Classifier',gb_wickets_train)\n",
    "\n",
    "# Test Score for Gradient Boosting Classifier \n",
    "gb_wickets_test = gbw.score(X_test_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Test Score for Gradient Boosting Classifier\n",
    "print('Test Score for Gradient Boosting Classifier',gb_wickets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here too there is Overfitting which needs to be corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5 Pruned Gradient Boosting Classifier <a id='gbwprune'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "loss = ['deviance','exponential']\n",
    "# Learning rate\n",
    "learning_rate = [1, 0.5, 0.25, 0.1, 0.05, 0.01]\n",
    "# No of estimators\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64]\n",
    "# Max features\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "# No of levels of trees\n",
    "max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "# Critetion \n",
    "criterion = ['friedman mse','mse','mae']\n",
    "\n",
    "# Parameters in the Grid for Randomized Search CV\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "                'learning_rate':learning_rate,\n",
    "                'criterion':criterion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   30.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   33.5s\n",
      "[Parallel(n_jobs=-1)]: Done 285 out of 300 | elapsed:   51.4s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   58.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=GradientBoostingClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['friedman mse', 'mse',\n",
       "                                                      'mae'],\n",
       "                                        'learning_rate': [1, 0.5, 0.25, 0.1,\n",
       "                                                          0.05, 0.01],\n",
       "                                        'max_depth': [2, 5, 10, 15],\n",
       "                                        'max_features': ['auto', 'sqrt',\n",
       "                                                         'log2'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [1, 2, 4, 8, 16, 32,\n",
       "                                                         64]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 888,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "gb_random = RandomizedSearchCV(estimator = gbw, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "gb_random.fit(X_wickets,y_wickets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 16,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 15,\n",
       " 'learning_rate': 1,\n",
       " 'criterion': 'mae'}"
      ]
     },
     "execution_count": 889,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "gb_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting, training and testing the model again after Pruning\n",
    "gbw_prune = GradientBoostingClassifier(criterion='mae',n_estimators=16,min_samples_split=5,min_samples_leaf=4,max_features='auto',max_depth=15,learning_rate=1)\n",
    "\n",
    "gbw_prune.fit(X_train_wickets,y_train_wickets)\n",
    "\n",
    "gbw_prune_pred_wickets = gbw_prune.predict(X_test_wickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score after Pruning: 0.9521531100478469\n",
      "Precision Score after Pruning: 0.984375\n",
      "Recall Score after Pruning: 0.9642857142857143\n",
      "F1 Score after Pruning: 0.9742268041237113\n",
      "ROC AUC Score after Pruning: 0.8667582417582417\n",
      "Train Score for Gradient Boosting Classifier after Pruning 1.0\n",
      "Test Score for Gradient Boosting Classifier after Pruning 0.9569377990430622\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score after Pruning\n",
    "gb_prune_wickets_accuracy = metrics.accuracy_score(gb_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Accuracy Score after Pruning\n",
    "print('Accuracy Score after Pruning:',gb_prune_wickets_accuracy)\n",
    "\n",
    "# Precision Score after Pruning\n",
    "gb_prune_wickets_precision = metrics.precision_score(gb_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Precision Score after Pruning\n",
    "print('Precision Score after Pruning:',gb_prune_wickets_precision)\n",
    "\n",
    "# Recall Score after Pruning\n",
    "gb_prune_wickets_recall = metrics.recall_score(gb_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Recall Score after Pruning\n",
    "print('Recall Score after Pruning:',gb_prune_wickets_recall)\n",
    "\n",
    "# F1 Score after Pruning\n",
    "gb_prune_wickets_f1score = metrics.f1_score(gb_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing F1 Score after Pruning\n",
    "print('F1 Score after Pruning:',gb_prune_wickets_f1score)\n",
    "\n",
    "# ROC AUC Score after Pruning\n",
    "gb_prune_wickets_aucrocscore = metrics.roc_auc_score(gb_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing ROC AUC Score after Pruning \n",
    "print('ROC AUC Score after Pruning:',gb_prune_wickets_aucrocscore)\n",
    "\n",
    "# Train Score for Gradient Boosting Classifier after Pruning\n",
    "gb_prune_wickets_train = gbw_prune.score(X_train_wickets,y_train_wickets)\n",
    "\n",
    "# Printing Train Score for Gradient Boosting Classifier \n",
    "print('Train Score for Gradient Boosting Classifier after Pruning',gb_prune_wickets_train)\n",
    "\n",
    "# Test Score for Gradient Boosting Classifier after Pruning\n",
    "gb_prune_wickets_test = gbw_prune.score(X_test_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Test Score for Gradient Boosting Classifier after Pruning\n",
    "print('Test Score for Gradient Boosting Classifier after Pruning',gb_wickets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well even after Pruning we are not able to recitfy Overfitting so lets focus on other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6 Support Vector Machine Classifier <a id='svmw'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting,training and testing the model\n",
    "svcw = SVC()\n",
    "\n",
    "svcw.fit(X_train_wickets,y_train_wickets)\n",
    "svc_pred_wickets = svcw.predict(X_test_wickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9234449760765551\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.9230769230769231\n",
      "F1 Score: 0.9600000000000001\n",
      "ROC AUC Score: 0.9615384615384616\n",
      "Train Score for Support Machine Classifier 0.957345971563981\n",
      "Test Score for Gradient Boosting Classifier 0.9760765550239234\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "svc_wickets_accuracy = metrics.accuracy_score(svc_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',svc_wickets_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "svc_wickets_precision = metrics.precision_score(svc_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',svc_wickets_precision)\n",
    "\n",
    "# Recall Score\n",
    "svc_wickets_recall = metrics.recall_score(svc_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',svc_wickets_recall)\n",
    "\n",
    "# F1 Score\n",
    "svc_wickets_f1score = metrics.f1_score(svc_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',svc_wickets_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "svc_wickets_aucrocscore = metrics.roc_auc_score(svc_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',svc_wickets_aucrocscore)\n",
    "\n",
    "# Train Score for Support Machine Classifier\n",
    "svc_wickets_train = svcw.score(X_train_wickets,y_train_wickets)\n",
    "\n",
    "# Printing Train Score for Support Machine Classifier\n",
    "print('Train Score for Support Machine Classifier',svc_wickets_train)\n",
    "\n",
    "# Test Score for Support Machine Classifier\n",
    "svc_wickets_test = svcw.score(X_test_runs,y_test_wickets)\n",
    "\n",
    "# Printing Test Score for Support Machine Classifier\n",
    "print('Test Score for Gradient Boosting Classifier',svc_runs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine draws a hyperplane to reduce overfitting unlike other algorithms, and here too Precision is 1.0 through which we can tell there are no False Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.7 KNearest Neighbors Classifier <a id='knnw'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting,training and testing the model\n",
    "knnw = KNeighborsClassifier()\n",
    "\n",
    "knnw.fit(X_train_wickets,y_train_wickets)\n",
    "knn_pred_wickets = knnw.predict(X_test_wickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9425837320574163\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.9411764705882353\n",
      "F1 Score: 0.9696969696969697\n",
      "ROC AUC Score: 0.9705882352941176\n",
      "Train Score for Support Machine Classifier 0.9597156398104265\n",
      "Test Score for Gradient Boosting Classifier 0.9425837320574163\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "knn_wickets_accuracy = metrics.accuracy_score(knn_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',knn_wickets_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "knn_wickets_precision = metrics.precision_score(knn_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',knn_wickets_precision)\n",
    "\n",
    "# Recall Score\n",
    "knn_wickets_recall = metrics.recall_score(knn_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',knn_wickets_recall)\n",
    "\n",
    "# F1 Score\n",
    "knn_wickets_f1score = metrics.f1_score(knn_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',knn_wickets_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "knn_wickets_aucrocscore = metrics.roc_auc_score(knn_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',knn_wickets_aucrocscore)\n",
    "\n",
    "# Train Score for KNearest Neighbors Classifier\n",
    "knn_wickets_train = knnw.score(X_train_wickets,y_train_wickets)\n",
    "\n",
    "# Printing Train Score for KNearest Neighbors Classifier\n",
    "print('Train Score for Support Machine Classifier',knn_wickets_train)\n",
    "\n",
    "# Test Score for KNearest Neighbors Classifier\n",
    "knn_wickets_test = knnw.score(X_test_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Test Score for KNearest Neighbors Classifier\n",
    "print('Test Score for Gradient Boosting Classifier',knn_wickets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As KNN works with distance criteria, this algorithm too prevents Overfitting, and here too there are no False Positives as Precision is 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.8 Bagging Classifier <a id='bagw'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting,training and testing the model\n",
    "bcw = BaggingClassifier()\n",
    "\n",
    "bcw.fit(X_train_wickets,y_train_wickets)\n",
    "bc_pred_wickets = bcw.predict(X_test_wickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9473684210526315\n",
      "Precision Score: 0.9791666666666666\n",
      "Recall Score: 0.9641025641025641\n",
      "F1 Score: 0.9715762273901809\n",
      "ROC AUC Score: 0.8391941391941392\n",
      "Train Score for Bagging Classifier 0.9976303317535545\n",
      "Test Score for Bagging Classifier 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "bc_wickets_accuracy = metrics.accuracy_score(bc_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',bc_wickets_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "bc_wickets_precision = metrics.precision_score(bc_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',bc_wickets_precision)\n",
    "\n",
    "# Recall Score\n",
    "bc_wickets_recall = metrics.recall_score(bc_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',bc_wickets_recall)\n",
    "\n",
    "# F1 Score\n",
    "bc_wickets_f1score = metrics.f1_score(bc_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',bc_wickets_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "bc_wickets_aucrocscore = metrics.roc_auc_score(bc_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',bc_wickets_aucrocscore)\n",
    "\n",
    "# Train Score for Bagging Classifier\n",
    "bc_wickets_train = bcw.score(X_train_wickets,y_train_wickets)\n",
    "\n",
    "# Printing Train Score for Bagging Classifier\n",
    "print('Train Score for Bagging Classifier',bc_wickets_train)\n",
    "\n",
    "# Test Score for Bagging Classifier\n",
    "bc_wickets_test = bcw.score(X_test_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Test Score for Bagging Classifier\n",
    "print('Test Score for Bagging Classifier',bc_wickets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Bagging Classifier reduces variance it thereby prevents Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.9 XG Boost Classifier <a id='xgbw'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting,training and testing the model\n",
    "xgbw= XGBClassifier()\n",
    "\n",
    "xgbw.fit(X_train_wickets,y_train_wickets)\n",
    "xgb_pred_wickets = xgbw.predict(X_test_wickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9521531100478469\n",
      "Precision Score: 0.9739583333333334\n",
      "Recall Score: 0.9739583333333334\n",
      "F1 Score: 0.9739583333333334\n",
      "ROC AUC Score: 0.8399203431372549\n",
      "Train Score for XG Boost Classifier 1.0\n",
      "Test Score for XG Boost Classifier 0.9521531100478469\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "xgb_wickets_accuracy = metrics.accuracy_score(xgb_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',xgb_wickets_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "xgb_wickets_precision = metrics.precision_score(xgb_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',xgb_wickets_precision)\n",
    "\n",
    "# Recall Score\n",
    "xgb_wickets_recall = metrics.recall_score(xgb_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',xgb_wickets_recall)\n",
    "\n",
    "# F1 Score\n",
    "xgb_wickets_f1score = metrics.f1_score(xgb_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',xgb_wickets_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "xgb_wickets_aucrocscore = metrics.roc_auc_score(xgb_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',xgb_wickets_aucrocscore)\n",
    "\n",
    "# Train Score for XG Boost Classifier\n",
    "xgb_wickets_train = xgbw.score(X_train_wickets,y_train_wickets)\n",
    "\n",
    "# Printing Train Score for XG Boost Classifier\n",
    "print('Train Score for XG Boost Classifier',xgb_wickets_train)\n",
    "\n",
    "# Test Score for XG Boost Classifier\n",
    "xgb_wickets_test = xgbw.score(X_test_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Test Score for XG Boost Classifier\n",
    "print('Test Score for XG Boost Classifier',xgb_wickets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prevent the Overfitting through Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.10 Pruned XG Boost Classifier <a id='xgbwprune'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning Grid for Randomized Search CV\n",
    "params_xgb_GS = {\"max_depth\": [3,5,6,7,8],\n",
    "              \"min_child_weight\" : [5,6,7,8],\n",
    "            'learning_rate':[0.05,0.1,0.2],\n",
    "            'n_estimators': [10,30,50,70]}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "xgb_random = RandomizedSearchCV(estimator = xgb, param_distributions = params_xgb_GS, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   40.9s\n",
      "[Parallel(n_jobs=-1)]: Done 216 tasks      | elapsed:   43.8s\n",
      "[Parallel(n_jobs=-1)]: Done 285 out of 300 | elapsed:   44.7s remaining:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   44.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                           colsample_bylevel=1,\n",
       "                                           colsample_bynode=1,\n",
       "                                           colsample_bytree=1, gamma=0,\n",
       "                                           gpu_id=-1, importance_type='gain',\n",
       "                                           interaction_constraints='',\n",
       "                                           learning_rate=0.300000012,\n",
       "                                           max_delta_step=0, max_depth=6,\n",
       "                                           min_child_weight=1, missing=nan,\n",
       "                                           monotone_constraints='()',\n",
       "                                           n_estimators=100, n_jobs=0,\n",
       "                                           num_parallel_tree=1, random_state=0,\n",
       "                                           reg_alpha=0, reg_lambda=1,\n",
       "                                           scale_pos_weight=1, subsample=1,\n",
       "                                           tree_method='exact',\n",
       "                                           validate_parameters=1,\n",
       "                                           verbosity=None),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'learning_rate': [0.05, 0.1, 0.2],\n",
       "                                        'max_depth': [3, 5, 6, 7, 8],\n",
       "                                        'min_child_weight': [5, 6, 7, 8],\n",
       "                                        'n_estimators': [10, 30, 50, 70]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the random search model\n",
    "xgb_random.fit(X_wickets,y_wickets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 10,\n",
       " 'min_child_weight': 7,\n",
       " 'max_depth': 3,\n",
       " 'learning_rate': 0.2}"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "xgb_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting,training and testing the model after Pruning\n",
    "xgbw_prune = XGBClassifier(n_estimators=10,min_child_weight=7,max_depth=3,learning_rate=0.3)\n",
    "\n",
    "xgbw_prune.fit(X_train_wickets,y_train_wickets)\n",
    "\n",
    "xgb_prune_pred_wickets = xgbw_prune.predict(X_test_wickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score after Pruning: 0.9282296650717703\n",
      "Precision Score after Pruning: 0.9479166666666666\n",
      "Recall Score after Pruning: 0.9732620320855615\n",
      "F1 Score after Pruning: 0.9604221635883904\n",
      "ROC AUC Score after Pruning: 0.7593582887700534\n",
      "Train Score for XG Boost Classifier after Pruning 0.966824644549763\n",
      "Test Score for XG Boost Classifier after Pruning 0.9282296650717703\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score after Pruning\n",
    "xgb_prune_wickets_accuracy = metrics.accuracy_score(xgb_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Accuracy Score after Pruning\n",
    "print('Accuracy Score after Pruning:',xgb_prune_wickets_accuracy)\n",
    "\n",
    "# Precision Score after Pruning\n",
    "xgb_prune_wickets_precision = metrics.precision_score(xgb_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Precision Score after Pruning\n",
    "print('Precision Score after Pruning:',xgb_prune_wickets_precision)\n",
    "\n",
    "# Recall Score after Pruning\n",
    "xgb_prune_wickets_recall = metrics.recall_score(xgb_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Recall Score after Pruning\n",
    "print('Recall Score after Pruning:',xgb_prune_wickets_recall)\n",
    "\n",
    "# F1 Score after Pruning\n",
    "xgb_prune_wickets_f1score = metrics.f1_score(xgb_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing F1 Score after Pruning\n",
    "print('F1 Score after Pruning:',xgb_prune_wickets_f1score)\n",
    "\n",
    "# ROC AUC Score after Pruning\n",
    "xgb_prune_wickets_aucrocscore = metrics.roc_auc_score(xgb_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing ROC AUC Score after Pruning\n",
    "print('ROC AUC Score after Pruning:',xgb_prune_wickets_aucrocscore)\n",
    "\n",
    "# Train Score for XG Boost Classifier after Pruning\n",
    "xgb_prune_wickets_train = xgbw_prune.score(X_train_wickets,y_train_wickets)\n",
    "\n",
    "# Printing Train Score for XG Boost Classifier after Pruning\n",
    "print('Train Score for XG Boost Classifier after Pruning',xgb_prune_wickets_train)\n",
    "\n",
    "# Test Score for XG Boost Classifier after Pruning\n",
    "xgb_prune_wickets_test = xgbw_prune.score(X_test_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Test Score for XG Boost Classifier after Pruning\n",
    "print('Test Score for XG Boost Classifier after Pruning',xgb_prune_wickets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have somewhat corrected Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.11 Ada Boost Classifier <a id='abw'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting,training and testing the model\n",
    "abw = AdaBoostClassifier()\n",
    "\n",
    "abw.fit(X_train_wickets,y_train_wickets)\n",
    "\n",
    "ab_pred_wickets = abw.predict(X_test_wickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9856459330143541\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.9846153846153847\n",
      "F1 Score: 0.9922480620155039\n",
      "ROC AUC Score: 0.9923076923076923\n",
      "Train Score for Ada Boost Classifier 1.0\n",
      "Test Score for Ada Boost Classifier 0.9856459330143541\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "ab_wickets_accuracy = metrics.accuracy_score(ab_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',ab_wickets_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "ab_wickets_precision = metrics.precision_score(ab_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',ab_wickets_precision)\n",
    "\n",
    "# Recall Score\n",
    "ab_wickets_recall = metrics.recall_score(ab_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',ab_wickets_recall)\n",
    "\n",
    "# F1 Score\n",
    "ab_wickets_f1score = metrics.f1_score(ab_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',ab_wickets_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "ab_wickets_aucrocscore = metrics.roc_auc_score(ab_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',ab_wickets_aucrocscore)\n",
    "\n",
    "# Train Score for Ada Boost Classifier\n",
    "ab_wickets_train = abw.score(X_train_wickets,y_train_wickets)\n",
    "\n",
    "# Printing Train Score for Ada Boost Classifier\n",
    "print('Train Score for Ada Boost Classifier',ab_wickets_train)\n",
    "\n",
    "# Test Score for Ada Boost Classifier\n",
    "ab_wickets_test = abw.score(X_test_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Test Score for Ada Boost Classifier\n",
    "print('Test Score for Ada Boost Classifier',ab_wickets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.12 Pruned Ada Boost Classifier <a id='abwprune'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the parameters to a Grid for performing Randomized Search CV\n",
    "params_Adb_GS = {'learning_rate':[0.05,0.1,0.2,1],'n_estimators':[10,20,30,40],'algorithm':['SAMME', 'SAMME.R']}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "ab_random = RandomizedSearchCV(estimator = ab,param_distributions=params_Adb_GS,n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:282: UserWarning: The total space of parameters 32 is smaller than n_iter=100. Running 32 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed:    2.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=AdaBoostClassifier(), n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'algorithm': ['SAMME', 'SAMME.R'],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.2, 1],\n",
       "                                        'n_estimators': [10, 20, 30, 40]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the Randomized Search CV\n",
    "ab_random.fit(X_wickets,y_wickets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 30, 'learning_rate': 0.2, 'algorithm': 'SAMME'}"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "ab_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting,training and testing the model after Pruning\n",
    "abw_prune = AdaBoostClassifier(n_estimators=30,learning_rate=0.2,algorithm='SAMME')\n",
    "\n",
    "abw_prune.fit(X_train_wickets,y_train_wickets)\n",
    "\n",
    "ab_prune_pred_wickets = abw_prune.predict(X_test_wickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score after Pruning: 0.9330143540669856\n",
      "Precision Score after Pruning: 0.953125\n",
      "Recall Score after Pruning: 0.973404255319149\n",
      "F1 Score after Pruning: 0.9631578947368422\n",
      "ROC AUC Score after Pruning: 0.7724164133738602\n",
      "Train Score for Ada Boost Classifier after Pruning 0.966824644549763\n",
      "Test Score for Ada Boost Classifier after Pruning 0.9330143540669856\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score after Pruning\n",
    "ab_prune_wickets_accuracy = metrics.accuracy_score(ab_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Accuracy Score after Pruning\n",
    "print('Accuracy Score after Pruning:',ab_prune_wickets_accuracy)\n",
    "\n",
    "# Precision Score after Pruning\n",
    "ab_prune_wickets_precision = metrics.precision_score(ab_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Precision Score after Pruning\n",
    "print('Precision Score after Pruning:',ab_prune_wickets_precision)\n",
    "\n",
    "# Recall Score after Pruning\n",
    "ab_prune_wickets_recall = metrics.recall_score(ab_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Recall Score after Pruning\n",
    "print('Recall Score after Pruning:',ab_prune_wickets_recall)\n",
    "\n",
    "# F1 Score after Pruning\n",
    "ab_prune_wickets_f1score = metrics.f1_score(ab_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing F1 Score after Pruning\n",
    "print('F1 Score after Pruning:',ab_prune_wickets_f1score)\n",
    "\n",
    "# ROC AUC Score after Pruning\n",
    "ab_prune_wickets_aucrocscore = metrics.roc_auc_score(ab_prune_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing ROC AUC Score after Pruning\n",
    "print('ROC AUC Score after Pruning:',ab_prune_wickets_aucrocscore)\n",
    "\n",
    "# Train Score for Ada Boost Classifier after Pruning\n",
    "ab_prune_wickets_train = abw_prune.score(X_train_wickets,y_train_wickets)\n",
    "\n",
    "# Printing Train Score for Ada Boost Classifier after Pruning\n",
    "print('Train Score for Ada Boost Classifier after Pruning',ab_prune_wickets_train)\n",
    "\n",
    "# Test Score for Ada Boost Classifier after Pruning\n",
    "ab_prune_wickets_test = abw_prune.score(X_test_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Test Score for Ada Boost Classifier after Pruning\n",
    "print('Test Score for Ada Boost Classifier after Pruning',ab_prune_wickets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prevented Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.13 Logistic Regression <a id='lrw'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Fitting,training and testing the model\n",
    "lrw = LogisticRegression()\n",
    "\n",
    "lrw.fit(X_train_wickets,y_train_wickets)\n",
    "\n",
    "lr_pred_wickets = lrw.predict(X_test_wickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9904306220095693\n",
      "Precision Score: 0.9947916666666666\n",
      "Recall Score: 0.9947916666666666\n",
      "F1 Score: 0.9947916666666666\n",
      "ROC AUC Score: 0.9679840686274509\n",
      "Train Score for Logistic Regression 1.0\n",
      "Test Score for Logistic Regression 0.9904306220095693\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "lr_wickets_accuracy = metrics.accuracy_score(lr_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',lr_wickets_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "lr_wickets_precision = metrics.precision_score(lr_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',lr_wickets_precision)\n",
    "\n",
    "# Recall Score\n",
    "lr_wickets_recall = metrics.recall_score(lr_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',lr_wickets_recall)\n",
    "\n",
    "# F1 Score\n",
    "lr_wickets_f1score = metrics.f1_score(lr_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',lr_wickets_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "lr_wickets_aucrocscore = metrics.roc_auc_score(lr_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',lr_wickets_aucrocscore)\n",
    "\n",
    "# Train Score for Logistic Regression\n",
    "lr_wickets_train = lrw.score(X_train_wickets,y_train_wickets)\n",
    "\n",
    "# Printing Train Score for Logistic Regression\n",
    "print('Train Score for Logistic Regression',lr_wickets_train)\n",
    "\n",
    "# Test Score for Logistic Regression\n",
    "lr_wickets_test = lrw.score(X_test_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Test Score for Logistic Regression\n",
    "print('Test Score for Logistic Regression',lr_wickets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see Overfitting, as Logistic Regression is a base model lets try using it as a meta classifier for Stacking to stack other models into it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.14 Stacking for Logistic Regression using Mlxtend Classifier <a id='stwmlx>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking KNN,SVM,XG Boost,Ada Boost and Gradient Boosting models into Logistic Regression for better results\n",
    "lrstackw = StackingClassifier(classifiers=[knnw,svcw,xgbw_prune,abw_prune,gbw_prune],meta_classifier=lrw)\n",
    "\n",
    "lrstackw.fit(X_train_wickets,y_train_wickets)\n",
    "\n",
    "lrstack_pred_wickets = lrstackw.predict(X_test_wickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score after Stacking for Logistic Regression: 0.9473684210526315\n",
      "Precision Score after Stacking for Logistic Regression: 0.984375\n",
      "Recall Score after Stacking for Logistic Regression: 0.9593908629441624\n",
      "F1 Score after Stacking for Logistic Regression: 0.9717223650385605\n",
      "ROC AUC Score after Stacking for Logistic Regression: 0.8546954314720813\n",
      "Train Score for Logistic Regression after Stacking for Logistic Regression 0.9881516587677726\n",
      "Test Score for Logistic Regression after Stacking for Logistic Regression 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score after Stacking for Logistic Regression\n",
    "lrstack_wickets_accuracy = metrics.accuracy_score(lrstack_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Accuracy Score after Stacking for Logistic Regression\n",
    "print('Accuracy Score after Stacking for Logistic Regression:',lrstack_wickets_accuracy)\n",
    "\n",
    "# Precision Score after Stacking for Logistic Regression\n",
    "lrstack_wickets_precision = metrics.precision_score(lrstack_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Precision Score after Stacking for Logistic Regression\n",
    "print('Precision Score after Stacking for Logistic Regression:',lrstack_wickets_precision)\n",
    "\n",
    "# Recall Score after Stacking for Logistic Regression\n",
    "lrstack_wickets_recall = metrics.recall_score(lrstack_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Recall Score after Stacking for Logistic Regression\n",
    "print('Recall Score after Stacking for Logistic Regression:',lrstack_wickets_recall)\n",
    "\n",
    "# F1 Score after Stacking for Logistic Regression\n",
    "lrstack_wickets_f1score = metrics.f1_score(lrstack_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing F1 Score after Stacking for Logistic Regression\n",
    "print('F1 Score after Stacking for Logistic Regression:',lrstack_wickets_f1score)\n",
    "\n",
    "# ROC AUC Score after Stacking for Logistic Regression\n",
    "lrstack_wickets_aucrocscore = metrics.roc_auc_score(lrstack_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing ROC AUC Score after Stacking for Logistic Regression\n",
    "print('ROC AUC Score after Stacking for Logistic Regression:',lrstack_wickets_aucrocscore)\n",
    "\n",
    "# Train Score for Logistic Regression after Stacking for Logistic Regression\n",
    "lrstack_wickets_train = lrstackw.score(X_train_wickets,y_train_wickets)\n",
    "\n",
    "# Printing Train Score for Logistic Regression after Stacking for Logistic Regression\n",
    "print('Train Score for Logistic Regression after Stacking for Logistic Regression',lrstack_wickets_train)\n",
    "\n",
    "# Test Score for Logistic Regression after Stacking for Logistic Regression\n",
    "lrstack_wickets_test = lrstackw.score(X_test_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Test Score for Logistic Regression after Stacking for Logistic Regression\n",
    "print('Test Score for Logistic Regression after Stacking for Logistic Regression',lrstack_wickets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Logistic Regression give better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.15 Stacking using Voting Classifier <a id='stwvote'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning estimator models for voting classifier\n",
    "vote_est = [('knn',knnw),('xgb',xgbw_prune),('SVM',svcw)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "votew = VotingClassifier(estimators=vote_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing the model\n",
    "votew.fit(X_train_wickets,y_train_wickets)\n",
    "\n",
    "vote_pred_wickets = votew.predict(X_test_wickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.937799043062201\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.9365853658536586\n",
      "F1 Score: 0.9672544080604535\n",
      "ROC AUC Score: 0.9682926829268292\n",
      "Train Score 0.9691943127962085\n",
      "Test Score 0.937799043062201\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "vote_wickets_accuracy = metrics.accuracy_score(vote_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Accuracy Score\n",
    "print('Accuracy Score:',vote_wickets_accuracy)\n",
    "\n",
    "# Precision Score\n",
    "vote_wickets_precision = metrics.precision_score(vote_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Precision Score\n",
    "print('Precision Score:',vote_wickets_precision)\n",
    "\n",
    "# Recall Score\n",
    "vote_wickets_recall = metrics.recall_score(vote_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Recall Score\n",
    "print('Recall Score:',vote_wickets_recall)\n",
    "\n",
    "# F1 Score\n",
    "vote_wickets_f1score = metrics.f1_score(vote_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing F1 Score\n",
    "print('F1 Score:',vote_wickets_f1score)\n",
    "\n",
    "# ROC AUC Score\n",
    "vote_wickets_aucrocscore = metrics.roc_auc_score(vote_pred_wickets,y_test_wickets)\n",
    "\n",
    "# Printing ROC AUC Score\n",
    "print('ROC AUC Score:',vote_wickets_aucrocscore)\n",
    "\n",
    "# Train Score\n",
    "vote_wickets_train = votew.score(X_train_wickets,y_train_wickets)\n",
    "\n",
    "# Printing Train Score\n",
    "print('Train Score',vote_wickets_train)\n",
    "\n",
    "# Test Score\n",
    "vote_wickets_test = votew.score(X_test_wickets,y_test_wickets)\n",
    "\n",
    "# Printing Test Score\n",
    "print('Test Score',vote_wickets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.16 Comparison Table of all models for Wickets Prediction <a id='ctw'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary with all the metrics\n",
    "wickets_metrics = {'Classifier': ['Decision Tree','Pruned Decision Tree','Random Forest','Gradient Boosting','Pruned Gradient Boosting','Support Vector Machine','KNN','Bagging','XG Boost','Pruned XG Boost','Ada Boost','Pruned Ada Boost','Logistic Regression','Stacking for Logistic Regression using Mlxtend','Stacking using Voting'],\n",
    "                'Accuracy':[dt_wickets_accuracy,dt_prune_wickets_accuracy,rf_wickets_accuracy,gb_wickets_accuracy,gb_prune_wickets_accuracy,svc_wickets_accuracy,knn_wickets_accuracy,bc_wickets_accuracy,xgb_wickets_accuracy,xgb_prune_wickets_accuracy,ab_wickets_accuracy,ab_prune_wickets_accuracy,lr_wickets_accuracy,lrstack_wickets_accuracy,vote_wickets_accuracy],\n",
    "                'Precision':[dt_wickets_precision,dt_prune_wickets_precision,rf_wickets_precision,gb_wickets_precision,gb_prune_wickets_precision,svc_wickets_precision,knn_wickets_precision,bc_wickets_precision,xgb_wickets_precision,xgb_prune_wickets_precision,ab_wickets_precision,ab_prune_wickets_precision,lr_wickets_precision,lrstack_wickets_precision,vote_wickets_precision],\n",
    "                'Recall':[dt_wickets_recall,dt_prune_wickets_recall,rf_wickets_recall,gb_wickets_recall,gb_prune_wickets_recall,svc_wickets_recall,knn_wickets_recall,bc_wickets_recall,xgb_wickets_recall,xgb_prune_wickets_recall,ab_wickets_recall,ab_prune_wickets_recall,lr_wickets_recall,lrstack_wickets_recall,vote_wickets_recall],\n",
    "                'F1 Score':[dt_wickets_f1score,dt_prune_wickets_f1score,rf_wickets_f1score,gb_wickets_f1score,gb_prune_wickets_f1score,svc_wickets_f1score,knn_wickets_f1score,bc_wickets_f1score,xgb_wickets_f1score,xgb_prune_wickets_f1score,ab_wickets_f1score,ab_prune_wickets_f1score,lr_wickets_f1score,lrstack_wickets_f1score,vote_wickets_f1score],\n",
    "                'AUCROC Score':[dt_wickets_aucrocscore,dt_prune_wickets_aucrocscore,rf_wickets_aucrocscore,gb_wickets_aucrocscore,gb_prune_wickets_aucrocscore,svc_wickets_aucrocscore,knn_wickets_aucrocscore,bc_wickets_aucrocscore,xgb_wickets_aucrocscore,xgb_prune_wickets_aucrocscore,ab_wickets_aucrocscore,ab_prune_wickets_aucrocscore,lr_wickets_aucrocscore,lrstack_wickets_aucrocscore,vote_wickets_aucrocscore],\n",
    "                'Train Score':[dt_wickets_train,dt_prune_wickets_train,rf_wickets_train,gb_wickets_train,gb_prune_wickets_train,svc_wickets_train,knn_wickets_train,xgb_wickets_train,bc_wickets_train,xgb_prune_wickets_train,ab_wickets_train,ab_prune_wickets_train,lr_wickets_train,lrstack_wickets_train,vote_wickets_train],\n",
    "                'Test Score':[dt_wickets_test,dt_prune_wickets_test,rf_wickets_test,gb_wickets_test,gb_prune_wickets_test,svc_wickets_test,knn_wickets_test,xgb_runs_test,bc_wickets_test,xgb_prune_wickets_test,ab_wickets_test,ab_prune_wickets_test,lr_wickets_test,lrstack_wickets_test,vote_wickets_test]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe with the dictionary\n",
    "wickets_metrics = pd.DataFrame(wickets_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>AUCROC Score</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.923445</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.963158</td>\n",
       "      <td>0.958115</td>\n",
       "      <td>0.744737</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pruned Decision Tree</td>\n",
       "      <td>0.913876</td>\n",
       "      <td>0.927083</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.951872</td>\n",
       "      <td>0.729752</td>\n",
       "      <td>0.947867</td>\n",
       "      <td>0.913876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.918660</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.918660</td>\n",
       "      <td>0.957606</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.947867</td>\n",
       "      <td>0.918660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.956938</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.974093</td>\n",
       "      <td>0.976623</td>\n",
       "      <td>0.862047</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pruned Gradient Boosting</td>\n",
       "      <td>0.952153</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.974227</td>\n",
       "      <td>0.866758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.923445</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.923445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.942584</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.959716</td>\n",
       "      <td>0.942584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.964103</td>\n",
       "      <td>0.971576</td>\n",
       "      <td>0.839194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XG Boost</td>\n",
       "      <td>0.952153</td>\n",
       "      <td>0.973958</td>\n",
       "      <td>0.973958</td>\n",
       "      <td>0.973958</td>\n",
       "      <td>0.839920</td>\n",
       "      <td>0.997630</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pruned XG Boost</td>\n",
       "      <td>0.928230</td>\n",
       "      <td>0.947917</td>\n",
       "      <td>0.973262</td>\n",
       "      <td>0.960422</td>\n",
       "      <td>0.759358</td>\n",
       "      <td>0.966825</td>\n",
       "      <td>0.928230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ada Boost</td>\n",
       "      <td>0.985646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.992248</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Pruned Ada Boost</td>\n",
       "      <td>0.933014</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.973404</td>\n",
       "      <td>0.963158</td>\n",
       "      <td>0.772416</td>\n",
       "      <td>0.966825</td>\n",
       "      <td>0.933014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.990431</td>\n",
       "      <td>0.994792</td>\n",
       "      <td>0.994792</td>\n",
       "      <td>0.994792</td>\n",
       "      <td>0.967984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Stacking for Logistic Regression using Mlxtend</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.959391</td>\n",
       "      <td>0.971722</td>\n",
       "      <td>0.854695</td>\n",
       "      <td>0.988152</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Stacking using Voting</td>\n",
       "      <td>0.937799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936585</td>\n",
       "      <td>0.967254</td>\n",
       "      <td>0.968293</td>\n",
       "      <td>0.969194</td>\n",
       "      <td>0.937799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Classifier  Accuracy  Precision  \\\n",
       "0                                    Decision Tree  0.923445   0.953125   \n",
       "1                             Pruned Decision Tree  0.913876   0.927083   \n",
       "2                                    Random Forest  0.918660   1.000000   \n",
       "3                                Gradient Boosting  0.956938   0.979167   \n",
       "4                         Pruned Gradient Boosting  0.952153   0.984375   \n",
       "5                           Support Vector Machine  0.923445   1.000000   \n",
       "6                                              KNN  0.942584   1.000000   \n",
       "7                                          Bagging  0.947368   0.979167   \n",
       "8                                         XG Boost  0.952153   0.973958   \n",
       "9                                  Pruned XG Boost  0.928230   0.947917   \n",
       "10                                       Ada Boost  0.985646   1.000000   \n",
       "11                                Pruned Ada Boost  0.933014   0.953125   \n",
       "12                             Logistic Regression  0.990431   0.994792   \n",
       "13  Stacking for Logistic Regression using Mlxtend  0.947368   0.984375   \n",
       "14                           Stacking using Voting  0.937799   1.000000   \n",
       "\n",
       "      Recall  F1 Score  AUCROC Score  Train Score  Test Score  \n",
       "0   0.963158  0.958115      0.744737     1.000000    0.923445  \n",
       "1   0.978022  0.951872      0.729752     0.947867    0.913876  \n",
       "2   0.918660  0.957606      0.801314     0.947867    0.918660  \n",
       "3   0.974093  0.976623      0.862047     1.000000    0.956938  \n",
       "4   0.964286  0.974227      0.866758     1.000000    0.942584  \n",
       "5   0.923077  0.960000      0.961538     0.957346    0.923445  \n",
       "6   0.941176  0.969697      0.970588     0.959716    0.942584  \n",
       "7   0.964103  0.971576      0.839194     1.000000    0.980861  \n",
       "8   0.973958  0.973958      0.839920     0.997630    0.947368  \n",
       "9   0.973262  0.960422      0.759358     0.966825    0.928230  \n",
       "10  0.984615  0.992248      0.992308     1.000000    0.985646  \n",
       "11  0.973404  0.963158      0.772416     0.966825    0.933014  \n",
       "12  0.994792  0.994792      0.967984     1.000000    0.990431  \n",
       "13  0.959391  0.971722      0.854695     0.988152    0.947368  \n",
       "14  0.936585  0.967254      0.968293     0.969194    0.937799  "
      ]
     },
     "execution_count": 903,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wickets_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing all the models KNN is the best model with least train test difference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
